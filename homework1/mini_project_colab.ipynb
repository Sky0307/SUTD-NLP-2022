{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf_8OI34CdXU"
      },
      "source": [
        "<img src=\"images/sutd.png\" alt=\"drawing\" style=\"width:300px;\"/>\n",
        "\n",
        "## <center>50.040 Natural Language Processing, Summer 2021<center>\n",
        "    \n",
        "<center>**Due 17 June 2021, 5pm** <center>\n",
        "Mini Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAN5MA5GCdXU"
      },
      "source": [
        "**Write your student ID and name**\n",
        "\n",
        "\n",
        "### STUDENT ID: 1004365 \n",
        "\n",
        "### Name: Lee Jet Xuen\n",
        "\n",
        "### Students with whom you have discussed (if any):    \n",
        "1. Tay Sze Chang 1004301\n",
        "2. Brandon Chong Wah Jin "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0iF5uWcCdXV"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Language models are very useful for a wide range of applications, e.g., speech recognition and machine translation. Consider a sentence consisting of words $x_1, x_2, …, x_m$, where $m$ is the length of the sentence, the goal of language modeling is to model the probability of the sentence, where $m \\geq 1$, $x_i \\in V $ and $V$ is the vocabulary of the corpus:\n",
        "$$p(x_1, x_2, …, x_m)$$\n",
        "In this project, we are going to explore both statistical language model and neural language model on the [Wikitext-2](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) datasets. Download wikitext-2 word-level data and put it under the ``data`` folder.\n",
        "\n",
        "## Statistical  Language Model\n",
        "\n",
        "A simple way is to view words as independent random variables (i.e., zero-th order Markovian assumption). The joint probability can be written as:\n",
        "$$p(x_1, x_2, …, x_m)=\\prod_{i=1}^m p(x_i)$$\n",
        "However, this model ignores the word order information, to account for which, under the first-order Markovian assumption, the joint probability can be written as:\n",
        "$$p(x_0, x_1, x_2, …, x_{m})= \\prod_{i=1}^{m}p(x_i \\mid x_{i-1})$$\n",
        "Under the second-order Markovian assumption, the joint probability can be written as:\n",
        "$$p(x_{-1}, x_0, x_1, x_2, …, x_{m})= \\prod_{i=1}^{m}p(x_i \\mid x_{i-2}, x_{i-1})$$\n",
        "Similar to what we did in HMM, we will assume that $x_{-1}=START, x_0=START, x_{m} = STOP$ in this definition, where $START, STOP$ are special symbols referring to the start and the end of a sentence.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5wC4lqKCdXW"
      },
      "source": [
        "### Parameter estimation\n",
        "\n",
        "Let's use $count(u)$ to denote the number of times the unigram $u$ appears in the corpus, use $count(v, u)$ to denote the number of times the bigram $v, u$ appears in the corpus, and $count(w, v, u)$ the times the trigram $w, v, u$ appears in the corpus, $u \\in V \\cup STOP$ and $w, v \\in V \\cup START$.\n",
        "\n",
        "And the parameters of the unigram, bigram and trigram models can be obtained using maximum likelihood estimation (MLE).\n",
        "\n",
        "- In the unigram model, the parameters can be estimated as: $$p(u) = \\frac {count(u)}{c}$$, where $c$ is the total number of words in the corpus.\n",
        "- In the bigram model, the parameters can be estimated as:\n",
        "$$p(u \\mid v) = \\frac{count(v, u)}{count(v)}$$\n",
        "- In the trigram model, the parameters can be estimated as:\n",
        "$$p(u \\mid w, v) = \\frac{count(w, v, u)}{count(w, v)}$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7dks8t0CdXW"
      },
      "source": [
        "### Smoothing the parameters\n",
        "#### Add-k Smoothing\n",
        "Note, it is likely that many parameters of bigram and trigram models will be 0 because the relevant bigrams and trigrams involved do not appear in the corpus. If you don't have a way to handle these 0 probabilities, all the sentences that include such bigrams or trigrams will have probabilities of 0.\n",
        "\n",
        "We'll use a Add-k Smoothing method to fix this problem, the smoothed parameters can be estimated as:\n",
        "\\begin{equation}\n",
        "p_{add-k}(u)= \\frac{count(u)+k}{c+k|V^*|}\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "p_{add-k}(u \\mid v)= \\frac{count(v, u)+k}{count(v)+k|V^*|}\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "p_{add-k}(u \\mid w, v)= \\frac{count(w, v, u)+k}{count(w, v)+k|V^*|}\n",
        "\\end{equation}\n",
        "\n",
        "where $k \\in (0, 1)$ is the parameter of this approach, and $|V^*|$ is the size of the vocabulary $V^*$, here $V^*= V \\cup STOP$. One way to choose the value of $k$ is by\n",
        "optimizing the perplexity of the development set, namely to choose the value that minimizes the perplexity.\n",
        "#### Interpolation\n",
        "There is another way for smoothing which is named as **interpolation**. In interpolation, we always mix the probability estimates from\n",
        "all the n-gram estimators, weighing and combining the trigram, bigram, and unigram counts. In simple linear interpolation, we combine different order n-grams by linearly interpolating all the models. Thus, we estimate the trigram probability $p(w_n|w_{n-2},w_{n-1})$ by mixing together the unigram, bigram, and trigram probabilities, each weighted by a $\\lambda$:\n",
        "\\begin{align}\n",
        "\\hat{p}(w_n|w_{n-2},w_{n-1}) = \\lambda_1p(w_n|w_{n-2},w_{n-1})+\\lambda_2p(w_n|w_{n-1})+\\lambda_3p(w_n)\n",
        "\\end{align}\n",
        "such that the $\\lambda$s sum to 1:\n",
        "\\begin{align}\n",
        "\\sum_i\\lambda_i=1\n",
        "\\end{align}\n",
        "In addition, $\\lambda_1,\\lambda_2,\\lambda_3\\geq 0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNVha_8UCdXX"
      },
      "source": [
        "### Perplexity\n",
        "\n",
        "Given a test set $D^{\\prime}$ consisting of sentences $X^{(1)}, X^{(2)}, …, X^{(|D^{\\prime}|)}$, each sentence $X^{(j)}$ consists of words $x_1^{(j)}, x_2^{(j)},…,x_{n_j}^{(j)}$, we can measure the probability of each sentence $X^{(j)}$, and the quality of the language model would be the probability it assigns to the entire set of test sentences, namely:\n",
        "\\begin{equation} \n",
        "\\prod_{j=1}^{|D^{\\prime}|}p(X^{(j)})\n",
        "\\end{equation}\n",
        "Let's define average $log_2$ probability as:\n",
        "\\begin{equation} \n",
        "l=\\frac{1}{c^{\\prime}}\\sum_{j=1}^{|D^{\\prime}|}log_2p(X^{(j)})\n",
        "\\end{equation}\n",
        "$c^{\\prime}$ is the total number of words in the test set, $|D^{\\prime}|$ is the number of sentences. And the perplexity is defined as:\n",
        "\\begin{equation} \n",
        "perplexity=2^{-l}\n",
        "\\end{equation}\n",
        "\n",
        "The lower the perplexity, the better the language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wZmQ6S4lwIX"
      },
      "outputs": [],
      "source": [
        "from collections import Counter, namedtuple\n",
        "import itertools\n",
        "import numpy as np\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqBkrMLflwIZ"
      },
      "outputs": [],
      "source": [
        "with open(\"./data/wikitext-2/wiki.train.tokens\", 'r', encoding='utf8') as f:\n",
        "    text = f.readlines()\n",
        "    train_sents = [line.lower().strip('\\n').split() for line in text]\n",
        "    train_sents = [s for s in train_sents if len(s)>0 and s[0] != '=']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kLElkj8lwIa",
        "outputId": "6169d481-b632-40f0-c58a-5f3ee700d88f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['the', 'game', 'began', 'development', 'in', '2010', ',', 'carrying', 'over', 'a', 'large', 'portion', 'of', 'the', 'work', 'done', 'on', 'valkyria', 'chronicles', 'ii', '.', 'while', 'it', 'retained', 'the', 'standard', 'features', 'of', 'the', 'series', ',', 'it', 'also', 'underwent', 'multiple', 'adjustments', ',', 'such', 'as', 'making', 'the', 'game', 'more', '<unk>', 'for', 'series', 'newcomers', '.', 'character', 'designer', '<unk>', 'honjou', 'and', 'composer', 'hitoshi', 'sakimoto', 'both', 'returned', 'from', 'previous', 'entries', ',', 'along', 'with', 'valkyria', 'chronicles', 'ii', 'director', 'takeshi', 'ozawa', '.', 'a', 'large', 'team', 'of', 'writers', 'handled', 'the', 'script', '.', 'the', 'game', \"'s\", 'opening', 'theme', 'was', 'sung', 'by', 'may', \"'n\", '.']\n"
          ]
        }
      ],
      "source": [
        "print(train_sents[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0vxL-WpCdXX"
      },
      "source": [
        "### Question 1 [code]\n",
        "1. Implement the function **\"compute_ngram\"** that computes n-grams in the corpus.\n",
        " (Do not take the START and STOP symbols into consideration for now.) \n",
        "2. List 10 most frequent unigrams, bigrams and trigrams as well as their counts.(Hint: use the built-in function .most_common in Counter class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "Hhmii8cClwIb"
      },
      "outputs": [],
      "source": [
        "from nltk import ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOyoT-QiCdXY"
      },
      "outputs": [],
      "source": [
        "def compute_ngram(sents, n):\n",
        "    '''\n",
        "    Compute n-grams that appear in \"sents\".\n",
        "    param:\n",
        "        sents: list[list[str]] --- list of list of word strings\n",
        "        n: int --- \"n\" gram\n",
        "    return:\n",
        "        ngram_set: set{str} --- a set of n-grams (no duplicate elements)\n",
        "        ngram_dict: dict{ngram: counts} --- a dictionary that maps each ngram to its number occurence in \"sents\";\n",
        "        This dict contains the parameters of our ngram model. E.g. if n=2, ngram_dict={('a','b'):10, ('b','c'):13}\n",
        "        \n",
        "        You may need to use \"Counter\", \"tuple\" function here.\n",
        "    '''\n",
        "    ngram_set = None\n",
        "    ngram_dict = None\n",
        "    ### YOUR CODE HERE\n",
        "    ngram_list = []\n",
        "    \n",
        "    for word in sents:\n",
        "        for i in range(len(word)-n+1):\n",
        "            ngram_list.append(tuple(word[i:i+n]))\n",
        "            \n",
        "    ngram_set = set(ngram_list)\n",
        "    ngram_dict = Counter(ngram_list)\n",
        "\n",
        "    ### END OF YOUR CODE\n",
        "    return ngram_set, ngram_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7DPkCZClwIc",
        "outputId": "4e37a7bd-2379-4925-dee8-32fd8bf73eae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "unigram: 28910\n",
            "bigram: 577343\n",
            "trigram: 1344047\n"
          ]
        }
      ],
      "source": [
        "unigram_set, unigram_dict = compute_ngram(train_sents, 1)\n",
        "print('unigram: %d' %(len(unigram_set)))\n",
        "bigram_set, bigram_dict = compute_ngram(train_sents, 2)\n",
        "print('bigram: %d' %(len(bigram_set)))\n",
        "trigram_set, trigram_dict = compute_ngram(train_sents, 3)\n",
        "print('trigram: %d' %(len(trigram_set)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XnTBFc-lwId",
        "outputId": "e59a401d-fe41-47c0-9828-b4f908238534"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------------------------------------------------------\n",
            "unigram:\n",
            "[(('the',), 130519), ((',',), 99763), (('.',), 73388), (('of',), 56743), (('<unk>',), 53951), (('and',), 49940), (('in',), 44876), (('to',), 39462), (('a',), 36140), (('\"',), 28285)]\n",
            "---------------------------------------------------------------------------------------------------------------------\n",
            "bigram:\n",
            "[(('of', 'the'), 17242), (('in', 'the'), 11778), ((',', 'and'), 11643), (('.', 'the'), 11274), ((',', 'the'), 8024), (('<unk>', ','), 7698), (('to', 'the'), 6009), (('on', 'the'), 4495), (('the', '<unk>'), 4389), (('and', 'the'), 4331)]\n",
            "---------------------------------------------------------------------------------------------------------------------\n",
            "trigram:\n",
            "[((',', 'and', 'the'), 1393), ((',', '<unk>', ','), 950), (('<unk>', ',', '<unk>'), 901), (('one', 'of', 'the'), 866), (('<unk>', ',', 'and'), 819), (('.', 'however', ','), 775), (('<unk>', '<unk>', ','), 745), (('.', 'in', 'the'), 726), (('.', 'it', 'was'), 698), (('the', 'united', 'states'), 666)]\n",
            "---------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# List 10 most frequent unigrams, bigrams and trigrams as well as their counts.\n",
        "### YOUR CODE HERE\n",
        "print(\"-\" * 117)\n",
        "print(\"unigram:\")\n",
        "print(unigram_dict.most_common(10))\n",
        "print(\"-\" * 117)\n",
        "print(\"bigram:\")\n",
        "print(bigram_dict.most_common(10))\n",
        "print(\"-\" * 117)\n",
        "print(\"trigram:\")\n",
        "print(trigram_dict.most_common(10))\n",
        "print(\"-\" * 117)\n",
        "### END OF YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVpoDgU7CdXb"
      },
      "source": [
        "### Question 2 [code]\n",
        "In this part, we take the START and STOP symbols into consideration. So we need to pad the **train_sents** as described in \"Statistical Language Model\" before we apply \"compute_ngram\" function. For example, given a sentence \"I like NLP\", in a bigram model, we need to pad it as \"START I like NLP STOP\", in a trigram model, we need to pad it as \"START START I like NLP STOP\". For unigram model, it should be paded as \"I like NLP STOP\".\n",
        "\n",
        "1. Implement the ``pad_sents`` function.\n",
        "2. Pad ``train_sents``.\n",
        "3. Apply ``compute_ngram`` function to these padded sents.\n",
        "4. Implement ``ngram_prob`` function. Compute the probability for each n-gram in the variable **ngrams** according equations in **\"Parameter estimation\"**. List down the n-grams that have 0 probability. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2d30ZFulwIe",
        "outputId": "a69931b5-ce30-47b1-c4af-77fc9a6ad90b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['the', 'computer'], ['go', 'to'], ['have', 'had'], ['and', 'the'], ['can', 'sea'], ['a', 'number', 'of'], ['with', 'respect', 'to'], ['in', 'terms', 'of'], ['not', 'good', 'bad'], ['first', 'start', 'with']]\n"
          ]
        }
      ],
      "source": [
        "###############################################\n",
        "ngrams = list()\n",
        "with open('data/ngram.txt','r') as f:\n",
        "    for line in f:\n",
        "        ngrams.append(line.strip('\\n').split())\n",
        "print(ngrams)\n",
        "###############################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PK-aCCkjlwIe"
      },
      "outputs": [],
      "source": [
        "START = '<START>'\n",
        "STOP = '<STOP>'\n",
        "###################################\n",
        "def pad_sents(sents, n):\n",
        "    '''\n",
        "    Pad the sents according to n.\n",
        "    params:\n",
        "        sents: list[list[str]] --- list of sentences.\n",
        "        n: int --- specify the padding type, 1-gram, 2-gram, or 3-gram.\n",
        "    return:\n",
        "        padded_sents: list[list[str]] --- list of padded sentences.\n",
        "    '''\n",
        "    padded_sents = None\n",
        "    ### YOUR CODE HERE\n",
        "#     padded_sents = [[START]*(n-1)+s+[STOP] if n>1 else s for s in sents]\n",
        "    padded_sents = []\n",
        "    for word in sents:\n",
        "        if n>1:\n",
        "            padded_sents.append([START]*(n-1) + word + [STOP])\n",
        "        else:\n",
        "            padded_sents.append(word)\n",
        "    ### END OF YOUR CODE\n",
        "    return padded_sents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aL4peGuplwIf"
      },
      "outputs": [],
      "source": [
        "uni_sents = pad_sents(train_sents, 1)\n",
        "bi_sents = pad_sents(train_sents, 2)\n",
        "tri_sents = pad_sents(train_sents, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXHXuD3blwIf"
      },
      "outputs": [],
      "source": [
        "unigram_set, unigram_dict = compute_ngram(uni_sents, 1)\n",
        "bigram_set, bigram_dict = compute_ngram(bi_sents, 2)\n",
        "trigram_set, trigram_dict = compute_ngram(tri_sents, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVEerHrSlwIg",
        "outputId": "3217a998-ce47-4f7b-e689-43f6716c2dee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(28910, 580825, 1363266)"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(unigram_set),len(bigram_set),len(trigram_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEwrSCdFlwIg",
        "outputId": "b7c7ed5d-dd43-4e1d-c97b-746f9ad320f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2007146\n"
          ]
        }
      ],
      "source": [
        "num_words = sum([v for _,v in unigram_dict.items()])\n",
        "print(num_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FU1eMEaelwIh"
      },
      "outputs": [],
      "source": [
        "def ngram_prob(ngram, num_words, unigram_dic, bigram_dic, trigram_dic):\n",
        "    '''\n",
        "    params:\n",
        "        ngram: list[str] --- a list that represents n-gram\n",
        "        num_words: int --- total number of words\n",
        "        unigram_dic: dict{ngram: counts} --- a dictionary that maps each 1-gram to its number of occurences in \"sents\";\n",
        "        bigram_dic: dict{ngram: counts} --- a dictionary that maps each 2-gram to its number of occurence in \"sents\";\n",
        "        trigram_dic: dict{ngram: counts} --- a dictionary that maps each 3-gram to its number occurence in \"sents\";\n",
        "    return:\n",
        "        prob: float --- probability of the \"ngram\"\n",
        "    '''\n",
        "    prob = None\n",
        "    ### YOUR CODE HERE\n",
        "    num = len(ngram)\n",
        "    \n",
        "    ngram = tuple(ngram)\n",
        "    \n",
        "    if num == 1:\n",
        "        prob = unigram_dic[ngram] / num_words\n",
        "    elif num == 2:\n",
        "        prob = bigram_dic[ngram] / unigram_dic[ngram[:-1]]\n",
        "    elif num == 3:\n",
        "        prob = trigram_dic[ngram] / bigram_dic[ngram[:-1]]\n",
        "    \n",
        "    \n",
        "    ### END OF YOUR CODE\n",
        "    return prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2r3v81xlwIh",
        "outputId": "fb033cf6-aa23-4cb2-a73b-2f0542fffdcb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9.960235674499498e-05"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ngram_prob(ngrams[0], num_words,unigram_dict, bigram_dict, trigram_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgg8S4szlwIh",
        "outputId": "13064d8c-2e6f-4581-c236-7b845000be16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['can', 'sea']\n",
            "['not', 'good', 'bad']\n",
            "['first', 'start', 'with']\n"
          ]
        }
      ],
      "source": [
        "### List down the n-grams that have 0 probability. \n",
        "### YOUR CODE HERE\n",
        "for ngram in ngrams:\n",
        "    if ngram_prob(ngram, num_words, unigram_dict, bigram_dict, trigram_dict) == 0:\n",
        "        print(ngram)\n",
        "### END OF YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kAezpJ9CdXd"
      },
      "source": [
        "### Question 3 [code]\n",
        "\n",
        "1. Implement ``add_k_smoothing_ngram`` function to estimate ngram probability with ``add-k`` smoothing technique.\n",
        "2. Implement ``interpolation_ngram`` function to estimate ngram probability with ``interpolation`` smoothing technique.\n",
        "3. Implement ``perplexity`` function to compute the perplexity of the corpus \"**valid_sents**\" according to \"**Perplexity**\" section. The computation of $p(X^{(j)})$ depends on the n-gram model you choose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMFsbO24lwIi"
      },
      "outputs": [],
      "source": [
        "with open('data/wikitext-2/wiki.valid.tokens', 'r', encoding='utf8') as f:\n",
        "    text = f.readlines()\n",
        "    valid_sents = [line.lower().strip('\\n').split() for line in text]\n",
        "    valid_sents = [s for s in valid_sents if len(s)>0 and s[0] != '=']\n",
        "\n",
        "uni_valid_sents = pad_sents(valid_sents, 1)\n",
        "bi_valid_sents = pad_sents(valid_sents, 2)\n",
        "tri_valid_sents = pad_sents(valid_sents, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-G7K_SealwIi"
      },
      "outputs": [],
      "source": [
        "def add_k_smoothing_ngram(ngram, k, num_words, unigram_dic, bigram_dic, trigram_dic):\n",
        "    '''\n",
        "    params:\n",
        "        ngram: list[str] --- a list that represents n-gram\n",
        "        k: float \n",
        "        num_words: int --- total number of words\n",
        "        unigram_dic: dict{ngram: counts} --- a dictionary that maps each 1-gram to its number of occurences in \"sents\";\n",
        "        bigram_dic: dict{ngram: counts} --- a dictionary that maps each 2-gram to its number of occurence in \"sents\";\n",
        "        trigram_dic: dict{ngram: counts} --- a dictionary that maps each 3-gram to its number occurence in \"sents\";\n",
        "    return:\n",
        "        s_prob: float --- probability of the \"ngram\"\n",
        "    '''\n",
        "    s_prob = None\n",
        "    V = len(unigram_dic)\n",
        "    ### YOUR CODE HERE\n",
        "    num = len(ngram)\n",
        "    \n",
        "    ngram = tuple(ngram)\n",
        "    \n",
        "    if num == 1:\n",
        "        s_prob = (unigram_dic[ngram] + k) / (num_words + k*V)\n",
        "    elif num == 2:\n",
        "        s_prob = (bigram_dic[ngram] + k) / (unigram_dic[ngram[:-1]] + k*V)\n",
        "    elif num == 3:\n",
        "        s_prob = (trigram_dic[ngram] + k) / (bigram_dic[ngram[:-1]] + k*V)\n",
        "\n",
        "    ### END OF YOUR CODE\n",
        "    return s_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q01ez3QSlwIj"
      },
      "outputs": [],
      "source": [
        "def interpolation_ngram(ngram, lam, num_words, unigram_dic, bigram_dic, trigram_dic):\n",
        "    '''\n",
        "    params:\n",
        "        ngram: list[str] --- a list that represents n-gram\n",
        "        lam: list[float] --- a list of length 3.lam[0], lam[1] and lam[2] are correspondence to trigram, bigram and unigram,repectively.\n",
        "                             If len(ngram) == 1, lam[0]=lam[1]=0, lam[2]=1. If len(ngram) == 2, lam[0]=0. lam[0]+lam[1]+lam[2] = 1.\n",
        "        num_words: int --- total number of words\n",
        "        unigram_dic: dict{ngram: counts} --- a dictionary that maps each 1-gram to its number of occurences in \"sents\";\n",
        "        bigram_dic: dict{ngram: counts} --- a dictionary that maps each 2-gram to its number of occurence in \"sents\";\n",
        "        trigram_dic: dict{ngram: counts} --- a dictionary that maps each 3-gram to its number occurence in \"sents\";\n",
        "    return:\n",
        "        s_prob: float --- probability of the \"ngram\"\n",
        "    '''\n",
        "    s_prob = None\n",
        "    ### YOUR CODE HERE\n",
        "    num = len(ngram)\n",
        "    \n",
        "    if num == 1:\n",
        "        s_prob = ngram_prob(ngram, \n",
        "                            num_words,\n",
        "                            unigram_dic,\n",
        "                            bigram_dic,\n",
        "                            trigram_dic)\n",
        "    elif num == 2:\n",
        "        x,y = ngram\n",
        "        s_prob = lam[1]*ngram_prob(ngram,\n",
        "                                   num_words,\n",
        "                                   unigram_dic,\n",
        "                                   bigram_dic,\n",
        "                                   trigram_dic) +lam[2]*ngram_prob(y,\n",
        "                                                                  num_words,\n",
        "                                                                  unigram_dic,\n",
        "                                                                  bigram_dic,\n",
        "                                                                  trigram_dic)\n",
        "    elif num == 3:\n",
        "        x,y,z = ngram\n",
        "        s_prob = lam[0]*ngram_prob(ngram,\n",
        "                                   num_words,\n",
        "                                   unigram_dic,\n",
        "                                   bigram_dic,\n",
        "                                   trigram_dic) + lam[1]*ngram_prob((y,z),\n",
        "                                                                  num_words,\n",
        "                                                                  unigram_dic,\n",
        "                                                                  bigram_dic,\n",
        "                                                                  trigram_dic) + lam[2]*ngram_prob(z,\n",
        "                                                                                                  num_words,\n",
        "                                                                                                  unigram_dic,\n",
        "                                                                                                  bigram_dic,\n",
        "                                                                                                  trigram_dic)\n",
        "    \n",
        "    ### END OF YOUR CODE\n",
        "    return s_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "nF4yrZxylwIj",
        "outputId": "ddb21090-d5bc-4904-c0f7-4c49355adef7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['a', 'number', 'of']\n",
            "0.5088478366553233 0.7254474091334492\n"
          ]
        }
      ],
      "source": [
        "add_k_prob = add_k_smoothing_ngram(ngrams[5], 0.01, num_words, unigram_dict, bigram_dict, trigram_dict)\n",
        "interpolation_prob = interpolation_ngram(ngrams[5], [0.6,0.3,0.1], num_words, unigram_dict, bigram_dict, trigram_dict)\n",
        "print(ngrams[5])\n",
        "print(add_k_prob, interpolation_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_cNz2SflwIk"
      },
      "outputs": [],
      "source": [
        "def perplexity(n, method, num_words, valid_sents, unigram_dic, bigram_dic, trigram_dic, k=0, lam=[0,0,1]):\n",
        "    '''\n",
        "    params:\n",
        "        n: int --- n-gram model you choose \n",
        "        method: int ---- method == 0, use add_k_smoothing; method != 0, use interpolation method.\n",
        "        num_words: int --- total number of words\n",
        "        valid_sents: list[list[str]] --- list of sentences\n",
        "        unigram_dic: dict{ngram: counts} --- a dictionary that maps each 1-gram to its number of occurences in \"sents\";\n",
        "        bigram_dic: dict{ngram: counts} --- a dictionary that maps each 2-gram to its number of occurence in \"sents\";\n",
        "        trigram_dic: dict{ngram: counts} --- a dictionary that maps each 3-gram to its number occurence in \"sents\";\n",
        "        k: float --- The parameter of add_k_smoothing\n",
        "        lam: list[float] --- a list of length 3. The parameter of interpolation. \n",
        "   return:\n",
        "        ppl: float --- perplexity of valid_sents\n",
        "    '''\n",
        "    ppl = None\n",
        "    ### YOUR CODE HERE\n",
        "    loss = 0\n",
        "    \n",
        "    for s in valid_sents:\n",
        "        for i in range(len(s)-n+1):\n",
        "            if method == 0:\n",
        "                loss += np.log(add_k_smoothing_ngram(s[i:i+n], k, num_words, unigram_dic, bigram_dic, trigram_dic))\n",
        "            else:\n",
        "                loss += interpolation_ngram(s[i:i+n], lam, num_words, unigram_dic, bigram_dic, trigram_dic)\n",
        "\n",
        "    \n",
        "    loss /= sum([len(o) for o in valid_sents])\n",
        "    ppl = np.exp(-loss)\n",
        "    \n",
        "    ### END OF YOUR CODE\n",
        "    return ppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmhMdP3dlwIk",
        "outputId": "7e2dc5d3-fab5-41ed-b200-2888768be201"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "840.7346887991206"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "perplexity(1, 0, num_words, uni_valid_sents, unigram_dict, bigram_dict, trigram_dict, k=0.1, lam=[0,0,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7KPOdWXlwIk"
      },
      "source": [
        "### Question 4 [code][written]\n",
        "1. Based on add-k smoothing method, try out different $k\\in [ 0.0001, 0.001, 0.01, 0.1, 0.5]$ and different n-gram model (unigram, bigram and trigram). Find the model and $k$ that gives the best perplexity on \"**valid_sents**\" (smaller is better).\n",
        "2. Based on interpolation method, try out different $\\lambda$ where $\\lambda_1 = \\lambda_2$ and $\\lambda_3\\in [0.1, 0.2, 0.4, 0.6, 0.8]$. Find the $\\lambda$ that gives the best perplexity on \"**valid_sents**\" (smaller is better).\n",
        "3. Based on the methods and parameters we provide, choose the method that peforms best on the validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOzhOgKUlwIk",
        "outputId": "fa47d3fb-4960-4e11-d081-94a79914381a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The best n value is 2 and k value is 0.01 with a score of 505.03474198080517.\n"
          ]
        }
      ],
      "source": [
        "n = [1,2,3]\n",
        "k = [0.0001, 0.001, 0.01, 0.1, 0.5]\n",
        "### YOUR CODE HERE (add-k smoothing method)\n",
        "\n",
        "best_result = sys.maxsize\n",
        "best_n = None\n",
        "best_k = None\n",
        "for kidx in k:\n",
        "    for nidx in n:\n",
        "        ppl  = perplexity(nidx, 0, num_words, valid_sents, unigram_dict, bigram_dict, trigram_dict, kidx)\n",
        "        if ppl < best_result:\n",
        "            best_result = ppl\n",
        "            best_k = kidx\n",
        "            best_n = nidx\n",
        "print(f\"The best n value is {best_n} and k value is {best_k} with a score of {best_result}.\")\n",
        "\n",
        "### END OF YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqE_RkKElwIl",
        "outputId": "f79a69fe-6d66-40bb-b294-f67b3ef30e0a"
      },
      "outputs": [
        {
          "ename": "ZeroDivisionError",
          "evalue": "division by zero",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "Input \u001b[0;32mIn [141]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m lam_i \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m lam)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      9\u001b[0m lamb_list \u001b[38;5;241m=\u001b[39m [lam_i, lam_i, lam]\n\u001b[0;32m---> 10\u001b[0m result  \u001b[38;5;241m=\u001b[39m \u001b[43mperplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_sents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munigram_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbigram_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrigram_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlamb_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m<\u001b[39m best_result:\n\u001b[1;32m     12\u001b[0m     best_result \u001b[38;5;241m=\u001b[39m result\n",
            "Input \u001b[0;32mIn [131]\u001b[0m, in \u001b[0;36mperplexity\u001b[0;34m(n, method, num_words, valid_sents, unigram_dic, bigram_dic, trigram_dic, k, lam)\u001b[0m\n\u001b[1;32m     23\u001b[0m             loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(add_k_smoothing_ngram(s[i:i\u001b[38;5;241m+\u001b[39mn], k, num_words, unigram_dic, bigram_dic, trigram_dic))\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m             loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43minterpolation_ngram\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munigram_dic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbigram_dic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrigram_dic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28mlen\u001b[39m(o) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m valid_sents])\n\u001b[1;32m     29\u001b[0m ppl \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mloss)\n",
            "Input \u001b[0;32mIn [129]\u001b[0m, in \u001b[0;36minterpolation_ngram\u001b[0;34m(ngram, lam, num_words, unigram_dic, bigram_dic, trigram_dic)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m num \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m     36\u001b[0m     x,y,z \u001b[38;5;241m=\u001b[39m ngram\n\u001b[0;32m---> 37\u001b[0m     s_prob \u001b[38;5;241m=\u001b[39m lam[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[43mngram_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mngram\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mnum_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                               \u001b[49m\u001b[43munigram_dic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mbigram_dic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mtrigram_dic\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m lam[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39mngram_prob((y,z),\n\u001b[1;32m     42\u001b[0m                                                               num_words,\n\u001b[1;32m     43\u001b[0m                                                               unigram_dic,\n\u001b[1;32m     44\u001b[0m                                                               bigram_dic,\n\u001b[1;32m     45\u001b[0m                                                               trigram_dic) \u001b[38;5;241m+\u001b[39m lam[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m*\u001b[39mngram_prob(z,\n\u001b[1;32m     46\u001b[0m                                                                                               num_words,\n\u001b[1;32m     47\u001b[0m                                                                                               unigram_dic,\n\u001b[1;32m     48\u001b[0m                                                                                               bigram_dic,\n\u001b[1;32m     49\u001b[0m                                                                                               trigram_dic)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m### END OF YOUR CODE\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m s_prob\n",
            "Input \u001b[0;32mIn [123]\u001b[0m, in \u001b[0;36mngram_prob\u001b[0;34m(ngram, num_words, unigram_dic, bigram_dic, trigram_dic)\u001b[0m\n\u001b[1;32m     21\u001b[0m     prob \u001b[38;5;241m=\u001b[39m bigram_dic[ngram] \u001b[38;5;241m/\u001b[39m unigram_dic[ngram[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m num \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m---> 23\u001b[0m     prob \u001b[38;5;241m=\u001b[39m \u001b[43mtrigram_dic\u001b[49m\u001b[43m[\u001b[49m\u001b[43mngram\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbigram_dic\u001b[49m\u001b[43m[\u001b[49m\u001b[43mngram\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m### END OF YOUR CODE\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prob\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ],
      "source": [
        "lambda_3 = [0.1, 0.2, 0.4, 0.6, 0.8]\n",
        "### YOUR CODE HERE (interpolation method)\n",
        "\n",
        "best_result = sys.maxsize\n",
        "best_lam = None\n",
        "\n",
        "for lam in lambda_3:\n",
        "    lam_12 = (1 - lam)/2\n",
        "    lamb_list = [lam_12, lam_12, lam]\n",
        "    result  = perplexity(3, 1, num_words, valid_sents, unigram_dict, bigram_dict, trigram_dict, lam = lamb_list)\n",
        "    if result < best_result:\n",
        "        best_result = result\n",
        "        best_lam = lamb_list\n",
        "\n",
        "print(f\"The best lambda value is {best_lam} with a score of {best_result}.\")\n",
        "### END OF YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74CRBZq2lwIl"
      },
      "source": [
        "Based on the methods and parameters we provide, choose the method that peforms best on the validation data (**write your answer**): "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o_yjh9lCdXg"
      },
      "source": [
        "### Question 5 [code]\n",
        "\n",
        "Evaluate the perplexity of the test data **test_sents** based on the best model you choose in **Question 4**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7hGcXgCCdXg"
      },
      "outputs": [],
      "source": [
        "with open('data/wikitext-2/wiki.test.tokens', 'r', encoding='utf8') as f:\n",
        "    text = f.readlines()\n",
        "    test_sents = [line.lower().strip('\\n').split() for line in text]\n",
        "    test_sents = [s for s in test_sents if len(s)>0 and s[0] != '=']\n",
        "\n",
        "uni_test_sents = pad_sents(test_sents, 1)\n",
        "bi_test_sents = pad_sents(test_sents, 2)\n",
        "tri_test_sents = pad_sents(test_sents, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeYE77g1lwIm"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END OF YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePSI8RDWCdXj"
      },
      "source": [
        "## Neural Language Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkoTco_jCdXj"
      },
      "source": [
        "<img src=\"images/LM.png\" alt=\"drawing\" style=\"width:500px;\"/>\n",
        "\n",
        "We will create a LSTM language model as shown in figure and train it on the Wikitext-2 dataset. \n",
        "The data generators (train\\_iter, valid\\_iter, test\\_iter) have been provided. \n",
        "The word embeddings together with the parameters in the LSTM model will be learned from scratch.\n",
        "\n",
        "[Pytorch](https://pytorch.org/tutorials/) and [torchtext](https://torchtext.readthedocs.io/en/latest/index.html#) are required in this part. Do not make any changes to the provided code unless you are requested to do so. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHlAZf-KlwIm"
      },
      "source": [
        "### Question 6 [code]\n",
        "- Implement the ``__init__`` function in ``LangModel`` class. *Note: the code implementation should allow switching between unidirectional LSTM and bidirectional LSTM easily*\n",
        "- Implement the ``forward`` function in ``LangModel`` class.\n",
        "- Complete the training code in train function and the testing code in test function.\n",
        "- Train two models - **Unidirectional LSTM** and **Bidirectional LSTM**. Compute the perplexity of the test data \"test_iter\" using the trained models. The test perplexity of both trained models should be below 150."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9Cqd6WMlwIm"
      },
      "source": [
        "**Important Note: Make sure that \"torchtext <= 0.11\", as newer version might have torchtext.legacy removed**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CH3V67dxlwIn",
        "outputId": "b3d22b3d-3c8a-4502-a77c-cfe6b52f130a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.10.0\n",
            "  Downloading torchtext-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 19.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (2.23.0)\n",
            "Collecting torch==1.9.0\n",
            "  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 2.7 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext==0.10.0) (4.2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2022.5.18.1)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.12.0\n",
            "    Uninstalling torchtext-0.12.0:\n",
            "      Successfully uninstalled torchtext-0.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.9.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.9.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.9.0 torchtext-0.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U torchtext==0.10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tF2GCRdFCdXk",
        "outputId": "6848628b-aa91-4116-9dbb-f6c32b01bf9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fed8e5d7d90>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "import torchtext\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchtext.legacy.datasets import WikiText2\n",
        "from torch import nn, optim\n",
        "from torchtext.legacy import data\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n",
        "torch.manual_seed(222)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "lt6JBdSnCdXn"
      },
      "outputs": [],
      "source": [
        "def tokenizer(text):\n",
        "    '''Tokenize a string to words'''\n",
        "    return word_tokenize(text)\n",
        "\n",
        "START = '<START>'\n",
        "STOP = '<STOP>'\n",
        "#Load and split data into three parts\n",
        "TEXT = data.Field(lower=True, tokenize=tokenizer, init_token=START, eos_token=STOP)\n",
        "train, valid, test = WikiText2.splits(TEXT) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHwu4VMVCdXq",
        "outputId": "dcce524c-e53a-49f8-cae2-34c184f09015"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 28907\n"
          ]
        }
      ],
      "source": [
        "#Build a vocabulary from the train dataset\n",
        "TEXT.build_vocab(train)\n",
        "print('Vocabulary size:', len(TEXT.vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ua9e-OBMCdXs"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "# the length of a text feeding to the RNN layer\n",
        "BPTT_LEN = 32           \n",
        "# train, validation, test data\n",
        "train_iter, valid_iter, test_iter = data.BPTTIterator.splits((train, valid, test),\n",
        "                                                                batch_size=BATCH_SIZE,\n",
        "                                                                bptt_len=BPTT_LEN,\n",
        "                                                                repeat=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_Cd8shXCdXy",
        "scrolled": true,
        "outputId": "bdf54a86-bd5e-4637-c646-419676e04281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of text tensor torch.Size([32, 64])\n",
            "Size of target tensor torch.Size([32, 64])\n"
          ]
        }
      ],
      "source": [
        "#Generate a batch of train data\n",
        "batch = next(iter(train_iter))\n",
        "text, target = batch.text, batch.target\n",
        "print('Size of text tensor',text.size())\n",
        "print('Size of target tensor',target.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "MF70XAWIlwIo"
      },
      "outputs": [],
      "source": [
        "class LangModel(nn.Module):\n",
        "    def __init__(self, lang_config):\n",
        "        super(LangModel, self).__init__()\n",
        "        self.vocab_size = lang_config['vocab_size']\n",
        "        self.emb_size = lang_config['emb_size']\n",
        "        self.hidden_size = lang_config['hidden_size']\n",
        "        self.num_layer = lang_config['num_layer']\n",
        "        \n",
        "        self.embedding = None\n",
        "        self.lstm = None\n",
        "        self.linear = None\n",
        "        \n",
        "        ### TODO: \n",
        "        ###    1. Initialize 'self.embedding' with nn.Embedding function and 2 variables we have initialized for you\n",
        "        ###    2. Initialize 'self.lstm' with nn.LSTM function and 4 variables we have initialized for you \n",
        "        ###    3. Initialize 'self.linear' with nn.Linear function and 2 variables we have initialized for you\n",
        "        ### Reference:\n",
        "        ###        https://pytorch.org/docs/stable/nn.html\n",
        "        \n",
        "        ### YOUR CODE HERE (3 lines)\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.emb_size)\n",
        "        self.rnn = nn.LSTM(self.emb_size, self.hidden_size, self.num_layer, bidirectional=lang_config['bidirectional'])\n",
        "        self.linear = nn.Linear(self.hidden_size, self.vocab_size)\n",
        "\n",
        "        ### END OF YOUR CODE\n",
        "        \n",
        "    def forward(self, batch_sents, hidden=None):\n",
        "        '''\n",
        "        params:\n",
        "            batch_sents: torch.LongTensor of shape (sequence_len, batch_size)\n",
        "        return:\n",
        "            normalized_score: torch.FloatTensor of shape (sequence_len, batch_size, vocab_size)\n",
        "        '''\n",
        "        normalized_score = None\n",
        "        hidden = hidden\n",
        "        ### TODO:\n",
        "        ###      1. Feed the batch_sents to self.embedding  \n",
        "        ###      2. Feed the embeddings to self.lstm. Remember to pass \"hidden\" into self.lstm, even if it is None. But we will \n",
        "        ###         use \"hidden\" when implementing greedy search.\n",
        "        ###      3. Apply linear transformation to the output of self.lstm\n",
        "        ###      4. Apply 'F.log_softmax' to the output of linear transformation\n",
        "        ###\n",
        "        ### YOUR CODE HERE (4 lines)\n",
        "        \n",
        "        x = self.embedding(batch_sents)\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        score = self.linear(out)\n",
        "        normalized_score = F.log_softmax(score, dim=-1)\n",
        "        \n",
        "        ### END OF YOUR CODE\n",
        "        return normalized_score, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "FUY9QJJSlwIo"
      },
      "outputs": [],
      "source": [
        "def train(model, train_iter, valid_iter, vocab_size, criterion, optimizer, num_epochs):\n",
        "    for n in range(num_epochs):\n",
        "        train_loss = 0\n",
        "        target_num = 0\n",
        "        model.train()\n",
        "        for batch in train_iter:\n",
        "            \n",
        "            text, targets = batch.text.to(device), batch.target.to(device)\n",
        "            loss = None\n",
        "            \n",
        "            ### we don't consider \"hidden\" here. So according to the default setting, \"hidden\" will be None\n",
        "            ### YOU CODE HERE (~5 lines)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            pred, hidden = model(text)\n",
        "            loss = criterion(pred.view(-1, vocab_size), targets.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            ### END OF YOUR CODE\n",
        "            ##########################################\n",
        "            train_loss += loss.item() * targets.size(0) * targets.size(1)\n",
        "            target_num += targets.size(0) * targets.size(1)\n",
        "\n",
        "        train_loss /= target_num\n",
        "\n",
        "        # monitor the loss of all the predictions\n",
        "        val_loss = 0\n",
        "        target_num = 0\n",
        "        model.eval()\n",
        "        for batch in valid_iter:\n",
        "            text, targets = batch.text.to(device), batch.target.to(device)\n",
        "            \n",
        "            prediction,_ = model(text)\n",
        "            loss = criterion(prediction.view(-1, vocab_size), targets.view(-1))\n",
        "            \n",
        "            val_loss += loss.item() * targets.size(0) * targets.size(1)\n",
        "            target_num += targets.size(0) * targets.size(1)\n",
        "        val_loss /= target_num\n",
        "\n",
        "        print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(n+1, train_loss, val_loss))   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "TxE6EZRdlwIo"
      },
      "outputs": [],
      "source": [
        "def test(model, vocab_size, criterion, test_iter):\n",
        "    '''\n",
        "    params: \n",
        "        model: LSTM model\n",
        "        test_iter: test data\n",
        "    return:\n",
        "        ppl: perplexity \n",
        "    '''\n",
        "    ppl = None\n",
        "    test_loss = 0\n",
        "    target_num = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_iter:\n",
        "            text, targets = batch.text.to(device), batch.target.to(device)\n",
        "\n",
        "            prediction,_ = model(text)\n",
        "            loss = criterion(prediction.view(-1, vocab_size), targets.view(-1))\n",
        "\n",
        "            test_loss += loss.item() * targets.size(0) * targets.size(1)\n",
        "            target_num += targets.size(0) * targets.size(1)\n",
        "\n",
        "        test_loss /= target_num\n",
        "        \n",
        "        ### Compute perplexity according to \"test_loss\"\n",
        "        ### Hint: Consider how the loss is computed.\n",
        "        ### YOUR CODE HERE(1 line)\n",
        "        \n",
        "        ppl = np.exp(test_loss)\n",
        "        \n",
        "        ### END OF YOUR CODE\n",
        "        return ppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Wejs7ZUOlwIp"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'vocab_size':vocab_size,\n",
        "    'emb_size':128,\n",
        "    'hidden_size':128,\n",
        "    'num_layer':1,\n",
        "    'bidirectional': False\n",
        "}\n",
        "\n",
        "LM = LangModel(config)\n",
        "LM = LM.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qqIKFV2VlwIp"
      },
      "outputs": [],
      "source": [
        "num_epochs=10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vocab_size = len(TEXT.vocab)\n",
        "\n",
        "criterion = nn.NLLLoss(reduction='mean')\n",
        "optimizer = optim.Adam(LM.parameters(), lr=1e-3, betas=(0.7, 0.99))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omj8PbphlwIp",
        "outputId": "60c4b3a7-ede0-4670-dda8-b598d5caf1ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: 6.0626, Validation Loss: 5.1756\n",
            "Epoch: 2, Training Loss: 5.3989, Validation Loss: 4.9562\n",
            "Epoch: 3, Training Loss: 5.1284, Validation Loss: 4.8596\n",
            "Epoch: 4, Training Loss: 4.9565, Validation Loss: 4.8106\n",
            "Epoch: 5, Training Loss: 4.8304, Validation Loss: 4.7828\n",
            "Epoch: 6, Training Loss: 4.7316, Validation Loss: 4.7654\n",
            "Epoch: 7, Training Loss: 4.6503, Validation Loss: 4.7539\n",
            "Epoch: 8, Training Loss: 4.5810, Validation Loss: 4.7466\n",
            "Epoch: 9, Training Loss: 4.5204, Validation Loss: 4.7430\n",
            "Epoch: 10, Training Loss: 4.4667, Validation Loss: 4.7423\n"
          ]
        }
      ],
      "source": [
        "train(LM, train_iter, valid_iter, vocab_size, criterion, optimizer, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bU-R32aYlwIp",
        "outputId": "5f84e398-0fe9-4143-a16f-a049229bc9ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99.49475529850511"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "test(LM, vocab_size, criterion, test_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "KMzID-JelwIp"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'vocab_size':vocab_size,\n",
        "    'emb_size':128,\n",
        "    'hidden_size':128,\n",
        "    'num_layer':1,\n",
        "    'bidirectional': True\n",
        "}\n",
        "\n",
        "biLSTM = LangModel(config)\n",
        "biLSTM = biLSTM.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "-YkxLAf2lwIq",
        "outputId": "cb7ebf40-85d1-4af0-eb7e-0b0d74f20136"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-00647e57dc91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbiLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-889654e56d7c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_iter, valid_iter, vocab_size, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Function MmBackward returned an invalid gradient at index 0 - got [2048, 128] but expected shape compatible with [2048, 256]"
          ]
        }
      ],
      "source": [
        "train(biLSTM, train_iter, valid_iter, vocab_size, criterion, optimizer, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtseTiFclwIq"
      },
      "outputs": [],
      "source": [
        "test(biLSTM, vocab_size, criterion, test_iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojJX-WxPlwIq"
      },
      "source": [
        "### Question 7 [code][written]\n",
        "<img src=\"images/greedy.png\" alt=\"drawing\" style=\"width:500px;\"/>\n",
        "\n",
        "When we use trained language model to generate a sentence given a start token, we can choose ``greedy search``.\n",
        "\n",
        "As shown above, ``greedy search`` algorithm will pick the token which has the highest probability and feed it to the language model as input in the next time step. The model will generate ``max_len`` number of tokens at most.\n",
        "\n",
        "- Implement ``word_greedy_search``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pt9NzL6jlwIq"
      },
      "outputs": [],
      "source": [
        "def word_greedy_search(model, start_token, max_len):\n",
        "    '''\n",
        "    param:\n",
        "        model: nn.Module --- language model\n",
        "        start_token: str --- e.g. 'he'\n",
        "        max_len: int --- max number of tokens generated\n",
        "    return:\n",
        "        strings: list[str] --- list of tokens, e.g., ['he', 'was', 'a', 'member', 'of',...]\n",
        "    '''\n",
        "    model.eval()\n",
        "    ID = TEXT.vocab.stoi[start_token]\n",
        "    strings = [start_token]\n",
        "    hidden = None\n",
        "    \n",
        "    ### You may find TEXT.vocab.itos useful.\n",
        "    ### YOUR CODE HERE\n",
        "    \n",
        "    predicts = torch.ones(1, 1).long().to(device) * ID\n",
        "    for idx in range(max_len):\n",
        "        \n",
        "        logits, hidden = model(predicts, hidden)\n",
        "        predicts = torch.argmax(logits[-1,:,:], dim=-1)\n",
        "        strings.append(TEXT.vocab.itos[predicts.cpu().numpy()[0]])\n",
        "        \n",
        "        if strings[-1] == '<eos>': \n",
        "            break\n",
        "        \n",
        "        predicts.unsqueeze_(0)\n",
        "\n",
        "    ### END OF YOUR CODE \n",
        "    print(strings)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFNv7KHWlwIr"
      },
      "outputs": [],
      "source": [
        "word_greedy_search(LM, 'he', 64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRrDUwGAlwIr"
      },
      "source": [
        "#### Review Question: Based on your understanding, can we use the **Bidirectional LSTM** for this language generation (decoding) task? Explain why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNSaNxoYlwIr"
      },
      "source": [
        "**write your explanation:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4f0CXs9lwIs"
      },
      "source": [
        "### Question 8 [code][written]\n",
        "- We will use the hidden vectors (the working memory) of LSTM as the contextual embeddings. Implement ``contextual_embedding`` function.\n",
        "- Use the ``contextual_embedding`` function to get the contextual embeddings of the word \"sink\" in four sequences \"wood does not sink in water\", \"a small water leak will sink the ship\", \"there are plates in the kitchen sink\" and \"the kitchen sink was full of dirty dishes\". Then calculate the cosine similarity of \"sink\" from each pair of sequences. Assume that $\\boldsymbol{w}_1$ and $\\boldsymbol{w}_2$ are embeddings of \"sink\" in sequences \"wood does not sink in water\" and \"a small water leak will sink the ship\" respectively. The cosine similarity can be calculated as \n",
        "\n",
        "\\begin{align}\n",
        "similarity = cos(\\theta) = \\frac{\\boldsymbol{w}^{\\rm T}_1\\boldsymbol{w}_2}{||\\boldsymbol{w}_1||_2||\\boldsymbol{w}_2||_2}\n",
        "\\end{align}\n",
        "Give the explanation of the results. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTibzE2dlwIs"
      },
      "outputs": [],
      "source": [
        "def contextual_embedding(model, sentence):\n",
        "    '''\n",
        "    params: \n",
        "        model: nn.Module --- language model\n",
        "        sentence -- list[str]: list of tokens, e.g., ['I', 'am',...]\n",
        "    return:\n",
        "        embeddings -- numpy array of shape (length of sentence, word embedding size)\n",
        "    '''\n",
        "    model.eval()\n",
        "    hidden = None\n",
        "    \n",
        "    ### YOUR CODE HERE \n",
        "    \n",
        "    \n",
        "    \n",
        "    ### END OF YOUR CODE\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXZVCuzUlwIs"
      },
      "outputs": [],
      "source": [
        "sink_seq1 = \"wood does not sink in water\"\n",
        "sink_seq2 = \"a small water leak will sink the ship\"\n",
        "sink_seq3 = \"there are plates in the kitchen sink\"\n",
        "sink_seq4 = \"the kitchen sink was full of dirty dishes\"\n",
        "\n",
        "### YOUR CODE HERE \n",
        "\n",
        "### END OF YOUR CODE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dFfnENolwIs"
      },
      "source": [
        "***write your explanation:***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtgjpKXhlwIs"
      },
      "source": [
        "#### Review Question: Based on your understanding, can we use the **Bidirectional LSTM** for this contextual embedding task? Explain why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZs8u2mjlwIt"
      },
      "source": [
        "**write your explanation:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cernoBq6CdX9"
      },
      "source": [
        "### Requirements:\n",
        "- This is an individual report.\n",
        "- Complete the code using Python.\n",
        "- List students with whom you have discussed if there are any.\n",
        "- Follow the honor code strictly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDOUuC7-CdX-"
      },
      "source": [
        "### Free GPU Resources\n",
        "We suggest that you run neural language models on machines with GPU(s). Google provides the free online platform [Colaboratory](https://colab.research.google.com/notebooks/welcome.ipynb), a research tool for machine learning education and research. It’s a Jupyter notebook environment that requires no setup to use as common packages have been  pre-installed. Google users can have access to a Tesla T4 GPU (approximately 15G memory). Note that when you connect to a GPU-based VM runtime, you are given a maximum of 12 hours at a time on the VM.\n",
        "\n",
        "It is convenient to upload local Jupyter Notebook files and data to Colab, please refer to the [tutorial](https://colab.research.google.com/notebooks/io.ipynb). \n",
        "\n",
        "In addition, Microsoft also provides the online platform [Azure Notebooks](https://notebooks.azure.com/help/introduction) for research of data science and machine learning, there are free trials for new users with credits."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "mini_project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}