{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fWSKqb5v3Zi"
      },
      "source": [
        "![sutd](sutd.png)\n",
        "## <center>50.040 Natural Language Processing, Summer 2022<center>\n",
        "<center><bold>Homework 3</bold>\n",
        "\n",
        "<center>Due 29 July 2022, 5pm<center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4xePxuSv3Zp"
      },
      "source": [
        "**Write your student ID and name**\n",
        "\n",
        "ID: 1004365\n",
        "\n",
        "Name: Lee Jet Xuen\n",
        "\n",
        "Students whom you have discussed with (if any):    \n",
        "Jerome Heng 1004115    \n",
        "Brandon Chong 1004104    \n",
        "Tay Sze Chang 1004301"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O98coj7Cv3Zq"
      },
      "source": [
        "### Requirements:\n",
        "- Use Python to complete this homework.\n",
        "- Follow the honor code strictly.\n",
        "- ***Use torch >= 1.5.1***\n",
        "- ***Use torchtext >= 0.6.0***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ucdfGjggv3Zr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext import data\n",
        "from collections import namedtuple\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# directory = r\"drive/MyDrive/50.040 NLP/homework3/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCnIsyS2wh2g",
        "outputId": "1e341a56-2192-43f0-8c2d-df2fce28943e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/50.040 NLP/homework3/\n",
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Xby2Cuklxcvl",
        "outputId": "34acbe1f-b8e6-4b1d-c75a-c574f6741c5d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/50.040 NLP/homework3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/50.040 NLP/homework3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EcLNPmIv3Zs"
      },
      "source": [
        "# Part 2: Neural Machine Translation [25 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8q24SGGv3Zt"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LCyVCRkiv3Zu"
      },
      "outputs": [],
      "source": [
        "STOP_TOKEN = '</s>'\n",
        "START_TOKEN = '<s>'\n",
        "UNK_TOKEN = '<unk>'\n",
        "PAD_TOKEN = '<pad>'\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, sent_pairs, src_word2idx, tgt_word2idx, tokenizer, max_len):\n",
        "        self.pairs = sent_pairs\n",
        "        self.src_w2i = src_word2idx\n",
        "        self.tgt_w2i = tgt_word2idx\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        src_ids = []\n",
        "        tgt_ids = []\n",
        "        src = self.pairs[idx].src\n",
        "        tgt = self.pairs[idx].tgt\n",
        "        \n",
        "        src_words = self.tokenizer(src)\n",
        "        tgt_words = self.tokenizer(tgt)\n",
        "        for i in src_words:\n",
        "            try:\n",
        "                idx = self.src_w2i[i]\n",
        "            except KeyError:\n",
        "                idx = self.src_w2i[UNK_TOKEN]\n",
        "            src_ids.append(idx)\n",
        "        for j in tgt_words:\n",
        "            try:\n",
        "                idx = self.tgt_w2i[j]\n",
        "            except KeyError:\n",
        "                idx = self.tgt_w2i[UNK_TOKEN]\n",
        "            tgt_ids.append(idx)\n",
        "        \n",
        "        src_length = len(src_ids)\n",
        "        tgt_length = len(tgt_ids)\n",
        "        if src_length < self.max_len:\n",
        "            src_ids = src_ids + [self.src_w2i[STOP_TOKEN]] + [self.src_w2i[PAD_TOKEN]] * (self.max_len - src_length - 1)\n",
        "            assert len(src_ids) == self.max_len\n",
        "            src_length += 1\n",
        "        else:\n",
        "            src_ids = src_ids[:self.max_len-1] + [self.src_w2i[STOP_TOKEN]]\n",
        "            src_length = self.max_len\n",
        "            \n",
        "        if tgt_length < self.max_len-1:\n",
        "            tgt_ids = [self.tgt_w2i[START_TOKEN]] + tgt_ids + [self.tgt_w2i[STOP_TOKEN]] +\\\n",
        "            [self.tgt_w2i[PAD_TOKEN]] * (self.max_len - tgt_length - 2)\n",
        "            assert len(tgt_ids) == self.max_len\n",
        "            tgt_length += 2\n",
        "        else:\n",
        "            tgt_ids = [self.tgt_w2i[START_TOKEN]] + tgt_ids[:self.max_len-2] + [self.tgt_w2i[STOP_TOKEN]]\n",
        "            tgt_length = self.max_len\n",
        "            \n",
        "        src_mask = np.zeros(self.max_len)\n",
        "        tgt_mask = np.zeros(self.max_len)\n",
        "        src_mask[:src_length] = 1\n",
        "        tgt_mask[:tgt_length] = 1\n",
        "\n",
        "        return torch.LongTensor(src_ids), torch.LongTensor(tgt_ids), torch.LongTensor([src_length]), \\\n",
        "        torch.LongTensor([tgt_length]),  torch.BoolTensor(src_mask), torch.BoolTensor(tgt_mask), tgt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bABNT-gv3Zw"
      },
      "source": [
        "# utils\n",
        "### Question 6\n",
        "Before we build our model, we need to preprocess our data. \n",
        "Implement ``read_corpus`` function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_-JoFQFFv3Zx"
      },
      "outputs": [],
      "source": [
        "Pair = namedtuple('Pair', ['src','tgt'])\n",
        "\n",
        "\n",
        "def read_corpus(data_path):\n",
        "    '''\n",
        "    param: \n",
        "    data_path: str --- path to the data file\n",
        "\n",
        "    return: \n",
        "    src: list[str] --- contains the source language sentences; each sentence is a string;\n",
        "    tgt: list[str] --- contains the target language sentences; each sentence is a string;\n",
        "    src_vocab: set(str) --- contains all the source language words appearing in the data file; each word is a string;\n",
        "    src_tgt: set(str) --- --- contains all the target language words appearing in the data file; each word is a string;\n",
        "\n",
        "    '''\n",
        "    with open(data_path, 'r', encoding='utf-8') as d:\n",
        "        data = d.readlines()\n",
        "        src, tgt = [], []\n",
        "        src_vocab, tgt_vocab = set(), set()\n",
        "\n",
        "        # 'data' is a list of strings; each element of this list represents a sentence which ended with \"\\n\" .\n",
        "        # Source language sentence (French) and target language sentence (English) are split by \"\\t\"\n",
        "        # Don't forget to remove the special \"\\n\" symbol of each sentence string\n",
        "        \n",
        "        ### YOUR CODE HERE\n",
        "\n",
        "        for i in data:\n",
        "          s, t = i.split(\"\\t\")\n",
        "          src.append(s.strip())\n",
        "          src_vocab = src_vocab.union(s.strip().split())\n",
        "\n",
        "          tgt.append(t.strip())\n",
        "          tgt_vocab = tgt_vocab.union(t.strip().split())\n",
        "          \n",
        "        ### END OF YOUR CODE\n",
        "        assert len(src) == len(tgt)\n",
        "        return src, tgt, src_vocab, tgt_vocab\n",
        "\n",
        "def lang_pairs(src, tgt):\n",
        "    pairs = []\n",
        "    for s,t in zip(src, tgt):\n",
        "        pairs.append(Pair(src=s, tgt=t))\n",
        "    return pairs\n",
        "\n",
        "def build_w2i(vocab):\n",
        "\n",
        "    w2i = {}\n",
        "    for i, w in enumerate(vocab):\n",
        "        w2i[w] = i\n",
        "    w2i[START_TOKEN] = len(w2i)\n",
        "    w2i[STOP_TOKEN] = len(w2i)\n",
        "    w2i[UNK_TOKEN] = len(w2i)\n",
        "    w2i[PAD_TOKEN] = len(w2i)\n",
        "\n",
        "    return w2i \n",
        "\n",
        "def build_i2w(w2i):\n",
        "    i2w = {}\n",
        "    for k,v in w2i.items():\n",
        "        i2w[v] = k    \n",
        "    return i2w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "plUCX23Xv3Zy"
      },
      "outputs": [],
      "source": [
        "train_src, train_tgt, src_vocab, tgt_vocab = read_corpus(r'data/part2/train')\n",
        "dev_src, dev_tgt, _, _ = read_corpus(r'data/part2/dev')\n",
        "test_src, test_tgt, _, _ = read_corpus(r'data/part2/test')\n",
        "\n",
        "train_sent_pairs = lang_pairs(train_src,train_tgt)\n",
        "dev_sent_pairs = lang_pairs(dev_src,dev_tgt)\n",
        "test_sent_pairs = lang_pairs(test_src,test_tgt)\n",
        "\n",
        "fr_w2i = build_w2i(src_vocab)\n",
        "en_w2i = build_w2i(tgt_vocab)\n",
        "en_i2w = build_i2w(en_w2i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46wx_ZvVv3Zy",
        "outputId": "513c984e-a835-4871-bc1f-35efa5b5cc40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40992 25370 140000 140000\n",
            "40996 25374 25374\n"
          ]
        }
      ],
      "source": [
        "print(len(src_vocab), len(tgt_vocab), len(train_src), len(train_tgt))\n",
        "print(len(fr_w2i), len(en_w2i), len(en_i2w))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ00W529v3Zz",
        "outputId": "e1d5ba37-5c87-457c-f3cd-186c13c1089b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"Elle ne voulait pas qu'il joue au poker.\",\n",
              " \"She didn't want him to play poker.\")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "train_src[0], train_tgt[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJL2cdEpv3Zz"
      },
      "source": [
        "# Model\n",
        "### Question 7\n",
        " Implement part of the ``__init__`` function in ``Encoder`` class and ``Decoder`` class.\n",
        " \n",
        "### Question 8\n",
        "Implement the ``__init__`` and ``forward`` function ``Attention`` class. This function will generate attention distribution $\\alpha_t$.\n",
        "\n",
        "### Question 9\n",
        "Implement the ``forward`` function in ``Encoder`` class . \n",
        "This function converts source sentences into word embedding tensors $X$,\n",
        "generates $h_1^{enc},h_2^{enc},...,h_m^{enc}$ and \n",
        "computes initial hidden state $h_0^{dec}$, and initial cell state $c_0^{dec}$.\n",
        "\n",
        "### Question 10\n",
        "Implement the ``forward`` function in ``Decoder`` class. \n",
        "This function constructs $\\bar{y}_t$ and runs the ``decode_one_step`` function \n",
        "over every time step of the input sentence.\n",
        "\n",
        "### Question  11\n",
        "Implement ``decode_one_step`` function in ``Decoder`` class. \n",
        "This function applies the decoder's LSTM Cell for a \n",
        "single time step, computing the encoding of the target word $h_t^{dec}$, \n",
        "the attention distribution $\\alpha_t$, attention output\n",
        "$a_t$ and the combined output $o_t$.\n",
        "Hint: You should be using the implemented \"Attention\" class for the computation of the attention distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KDvMVqWvv3Z0"
      },
      "outputs": [],
      "source": [
        "BeamNode = namedtuple('BeamNode',['prev_node', 'prev_hidden','prev_o_t', 'wordID', 'score', 'length'])\n",
        "Translation = namedtuple('Translation',['sent', 'score'])\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available else 'cpu'\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, encoder_config):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = encoder_config['hidden_size']\n",
        "        self.num_layers = encoder_config['num_layers']\n",
        "        self.bidir = encoder_config['bidirectional']\n",
        "        self.vocab_size = encoder_config['vocab_size']\n",
        "        self.emb_size = encoder_config['emb_size']\n",
        "        self.src_emb_matrix = encoder_config['src_embedding']\n",
        "\n",
        "        self.scr_embedding = None\n",
        "        self.W_h = None\n",
        "        self.W_c = None\n",
        "\n",
        "        ### TODO Initialize variables:\n",
        "        #               self.scr_embedding: Embedding layer for source language\n",
        "        #               self.W_h: Linear layer without bias (W_h describled in the PDF)\n",
        "        #               self.W_c: Linear layer without bias (W_c described in the PDF)\n",
        "        #\n",
        "        #   You need to use nn.Embedding function and two variables we have initialized for you.\n",
        "        #   You need to use nn.Linear function and one variable we have initialized for you.\n",
        "        #   For the use of nn.Embedding function, please refer to https://pytorch.org/docs/stable/nn.html#torch.nn.embedding\n",
        "        #   For the use of nn.Linear function, please refer to https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n",
        "        #   In nn.Linear function, the matrix multiplication is a transposed version of the Eq.(1) in description PDF.\n",
        "        \n",
        "        ### YOUR CODE HERE (3 lines)\n",
        "        \n",
        "        self.src_embedding = nn.Embedding(self.vocab_size, self.emb_size)\n",
        "        self.W_h = nn.Linear(self.hidden_size * 2, self.hidden_size, bias=False)\n",
        "        self.W_c = nn.Linear(self.hidden_size * 2, self.hidden_size, bias=False)\n",
        "\n",
        "        ### END OF YOUR CODE\n",
        "\n",
        "        if self.src_emb_matrix is not None:\n",
        "            self.src_embedding.weight.data.copy_(torch.FloatTensor(self.src_emb_matrix))\n",
        "            self.src_embedding.weight.requires_grad = True\n",
        "        \n",
        "        self.rnn = nn.LSTM(input_size = self.emb_size,\n",
        "                           hidden_size = self.hidden_size,\n",
        "                           num_layers = self.num_layers,\n",
        "                           bidirectional = self.bidir,\n",
        "                           batch_first  = True)\n",
        "\n",
        "    def forward(self, src_ids, src_length):\n",
        "        '''\n",
        "        params:\n",
        "            src_ids: torch.LongTensor of shape (batch_size, max_len) \n",
        "            src_length: torch.LongTensor of shape (batch_size,) contains the actual length of each sentence in the batch\n",
        "        return:\n",
        "            encoder_outputs: torch.FloatTensor of shape(batch_size, max_len_in_batch, 2*hidden_size); the hidden states produced by Bi-LSTM\n",
        "            decoder_init: tuple(last_hidden, last_cell); last_hidden: torch.FloatTensorof shape (batch_size, 2*hidden_size); \n",
        "                                                        last_cell: torch.FloatTensor of shape(batch_size, 2*hidden_size); \n",
        "                                                        they are h_0^{dec},c_0^{dec} in our description PDF \n",
        "        '''\n",
        "\n",
        "        encoder_outputs, decoder_init = None, None\n",
        "        src_length = torch.as_tensor(src_length, dtype=torch.int64, device='cpu').squeeze(1)\n",
        "\n",
        "        ### TODO:\n",
        "        ###     1. feed the \"src_ids\" into the src embedding layer to get a tensor X of shape (batch_size, max_len, emb_size)\n",
        "        ###     2. apply \"pack_padded_sequence\" function to X to get a new tensor X_packed\n",
        "        ###        (tip: set batch_first=True, enforced_sorted=False in the pack_padded_sequence function)\n",
        "        ###     3. use Bi-LSTM (rnn) to encode  \"X_packed\" to get \"encoder_outputs\", \"last_hidden\", \"last_cell\"\n",
        "        ###     4. apply \"pad_packed_sequence\" to encoder_outputs (remember to set batch_first=True); \n",
        "        ###     5. note that last_hidden/last_cell is of shape (2, batch_size, hidden_size); \n",
        "        ###        we want a shape of (batch_size, 2*hidden_size)\n",
        "        ###     6. apply linear transformation W_h, W_c to last_hidden/last cell to get the initial decoder hidden state\n",
        "        ###        (batch_size, hidden_size) and initial decoder cell state (batch_size, hidden_size).\n",
        "        ### You may use these functions in your implemetation:\n",
        "        ###     pack_padded_sequence: https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence\n",
        "        ###     pad_packed_sequence: https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_packed_sequence\n",
        "        ###     torch.cat: https://pytorch.org/docs/stable/torch.html#torch.cat\n",
        "\n",
        "        ### YOUR CODE HERE (~ 9 lines)\n",
        "        \n",
        "        tensor_x = self.src_embedding(src_ids)\n",
        "        tensor_x_packed = pack_padded_sequence(tensor_x, src_length, batch_first=True, enforce_sorted=False)\n",
        "        encoder_out, l = self.rnn(tensor_x_packed)\n",
        "        h, c = l\n",
        "        encoder_outputs, _ = pad_packed_sequence(encoder_out, batch_first=True)\n",
        "        \n",
        "        h = torch.cat((h[0,:,:], h[1,:,:]), dim=1)\n",
        "        c = torch.cat((c[0,:,:], c[1,:,:]), dim=1)\n",
        "\n",
        "        h = self.W_h(h)\n",
        "        c = self.W_c(c)\n",
        "        \n",
        "        decoder_init = (h, c)\n",
        "\n",
        "        ### END OF YOUR CODE\n",
        "        return encoder_outputs, decoder_init\n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, decoder_config):\n",
        "        super(Decoder,self).__init__()\n",
        "        self.hidden_size = decoder_config['hidden_size']\n",
        "        self.vocab_size = decoder_config['vocab_size']\n",
        "        self.emb_size = decoder_config['emb_size']\n",
        "        self.tgt_emb_matrix = decoder_config['tgt_embedding']\n",
        "\n",
        "        self.rnn = None\n",
        "        self.W_u = None\n",
        "        self.tgt_embedding = None\n",
        "        self.attention = Attention(self.hidden_size)\n",
        "\n",
        "        ### TODO Initialize variables: \n",
        "        #               self.tgt_embedding: nn.Embedding layer for source language; You need to use nn.Embedding function \n",
        "        #                                  and 2 variables we have initialized for you.\n",
        "        #               self.rnn: nn.LSTMCell ; You need to use nn.LSTMCell function and 2 variables we have initialized for you.\n",
        "        #               self.W_u: nn.Linear layer without bias (W_u describled in the PDF)\n",
        "\n",
        "        # For the use of nn.Embedding function, please refer to https://pytorch.org/docs/stable/nn.html#\n",
        "        # For the use of nn.Linear function, please refer to https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n",
        "        # In nn.Linear function, the matrix multiplication is a transposed version of the Eq.(1) in description PDF.\n",
        "        # For the use of nn.LSTMCell function, please refer to https://pytorch.org/docs/stable/nn.html#lstmcell\n",
        "        # Think about the shape of \\bar{y}_t in the description PDF when initializing self.rnn with nn.LSTMCell\n",
        "\n",
        "        ### YOUR CODE HERE (4 lines)\n",
        "        \n",
        "        self.tgt_embedding = nn.Embedding(self.vocab_size, self.emb_size)\n",
        "        self.rnn = nn.LSTMCell(self.emb_size + self.hidden_size, self.hidden_size)\n",
        "        self.W_u = nn.Linear(3 * self.hidden_size, self.hidden_size, bias=False)\n",
        "                             \n",
        "        ### END OF YOUR CODE\n",
        "\n",
        "        if self.tgt_emb_matrix is not None:\n",
        "          self.tgt_embedding.weight.data.copy_(torch.Tensor(self.tgt_emb_matrix))\n",
        "          self.tgt_embedding.weight.requires_grad = True        \n",
        "\n",
        "    def forward(self, tgt_ids, tgt_lengths, encoder_outputs, encoder_output_masks, decoder_init):\n",
        "        '''\n",
        "        params:\n",
        "            tgt_ids: torch.LongTensor of shape (batch_size, max_len); each element is a number specifying the position of\n",
        "                    a word in a embedding matrix\n",
        "            tgt_lengths: torch.LongTensor of shape (batch_size,) contains the actual length of each sentence in the batch\n",
        "            encoder_outputs: torch.FloatTensosr of shape ( batch_size, max_len_in_batch, 2*hidden_size); \n",
        "                                \"max_len_in_batch\" is the max length in a batch. It is less than \"max_len\".\n",
        "            encoder_output_masks: torch.BoolTensor of shape (batch_size, max_len), specifying which positions are pad tokens.\n",
        "            decoder_init: tuple(h_0, c_0); the output \"decoder_init\" of the encoder; \n",
        "                            h_0 of shape (batch_size, hidden_size), c_0 of shape (batch_size, hidden_size)\n",
        "        return:\n",
        "            combined_outputs: torch.FloatTensor of shape (max_len_batch, batch_size, hidden_size)\n",
        "        '''\n",
        "        \n",
        "        decoder_state = decoder_init\n",
        "        max_len_batch = torch.max(tgt_lengths) -1               # don't consider the end token\n",
        "        batch_size = encoder_outputs.size()[0]\n",
        "        o_prev = torch.zeros(batch_size, self.hidden_size, device='cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        \n",
        "        combined_outputs = []\n",
        "        \n",
        "        ### TODO:\n",
        "        ###     1. feed the \"tgt_ids\" into the embedding layer to get a tensor \"Y\" of shape (batch_size, max_len, emb_size)\n",
        "        ###     2. construct a for loop with range 0:max_len_batch\n",
        "        ###         within the for loop: \n",
        "        ###                         1). slice Y by indexing; you should have y_t of shape (batch_size, emb_size)\n",
        "        ###                         2). concatenate y_t with o_prev , yielding ybar_t as described in the PDF\n",
        "        ###                         3). feed ybar_t and \"decoder state\", \"encoder_outputs\", \"encoder_output_masks\" into function \"decode_one_step()\"\n",
        "        ###                             and it will output new \"decoder_state\" (a tuple), new \"o_t\" \n",
        "        ###                         4). append new \"o_t\" to \"combined_outputs\"\n",
        "        ###                         5). update \"o_prev\" with new \"o_t\"\n",
        "        ###     3. use \"torch.stack\" function to process combined_outputs (a list of tensors; each tensor of shape (batch_size, hidden_size)) to \n",
        "        ###          a single tensor of shape (max_len_batch, batch_size, hidden_size)\n",
        "        ###\n",
        "        ### You may use these functions in your implementation:\n",
        "        ###     torch.cat: https://pytorch.org/docs/stable/torch.html#torch.cat\n",
        "        ###     torch.stack: https://pytorch.org/docs/stable/torch.html#torch.stack\n",
        "        ### YOUR CODE HERE (~ 8 lines)\n",
        "        \n",
        "        Y = self.tgt_embedding(tgt_ids)\n",
        "\n",
        "        for i in range(0, max_len_batch):\n",
        "          ybar_t = torch.cat((Y[:,i,:], o_prev), dim=1)\n",
        "          decoder_state, o_t = self.decode_one_step(ybar_t, \n",
        "                                                    decoder_state,\n",
        "                                                    encoder_outputs,\n",
        "                                                    encoder_output_masks)\n",
        "          combined_outputs.append(o_t)\n",
        "          o_prev = o_t\n",
        "\n",
        "        combined_outputs = torch.stack(combined_outputs,\n",
        "                                       dim=0)\n",
        "\n",
        "        ### END OF YOUR CODE\n",
        "        return combined_outputs       \n",
        "            \n",
        "    def decode_one_step(self, ybar_t, decoder_state, encoder_outputs, encoder_output_masks):\n",
        "        '''\n",
        "        param:\n",
        "            ybar_t: torch.FloatTensor of shape (batch_size, emb_size + hidden_size)\n",
        "            decoder_state: tuple(h_t, c_t); h_t of shape (batch_size, hidden_size); c_t of shape (batch_size, hidden_size);\n",
        "            encoder_hiddens: torch.FloatTensosr of shape ( batch_size, max_len_in_batch, 2*hidden_size); \"max_len_in_batch\" is the max length in a batch. It is less than \"max_len\".\n",
        "            encoder_hidden_masks: torch.BoolTensor of shape (batch_size, max_len), specifying which positions are pad tokens.\n",
        "        return: \n",
        "            decoder_state: tuple(h_t, c_t); both h_t and c_t have a shape (batch_size, hidden_size)\n",
        "            o_t: torch.FloatTensor of shape (batch_size, hidden_size)\n",
        "        '''\n",
        "        ### TODO:\n",
        "        ###     1. Apply the decoder (self.rnn) to \"ybar_t\", \"decoder_state\", yielding a new \"decoder_state\"\n",
        "        ###     2. split the decoder state into two parts, \"h\" and \"c\"; h has a shape (batch_size, hidden_size); c has a shape (batch_size, hidden_size)\n",
        "        ###     3. apply the \"Attention\" module to \"h\", \"encoder_outputs\", \"encoder_output_masks\", yielding attention weights (alpha_t in the PDF) of shape (batch_size, max_len_in_batch)\n",
        "        ###     4. apply torch.bmm function to alpha_t and \"encoder_hiddens\", yielding the \"a_t\" in PDF. \n",
        "        ###        You also need to use \"unsqueeze\" and \"squeeze\" function here. Be sure to specify the \"dim\" parameter in these two functions. \n",
        "        ###        \"a_t\" has a shape (batch_size, 2*hidden_size)\n",
        "        ###     5. concatenate \"a_t\" and \"h\", yielding \"u_t\" in the PDF; \"u_t\" has a shape (batch_size, 3*hidden_size)\n",
        "        ###     6. apply linear transformation W_u and \"torch.tanh\" function to \"u_t\", yielding \"o_t\" of shape (batch_size, hidden_size)\n",
        "\n",
        "        ### You may use these functions in your implementation:\n",
        "        ###     torch.cat: https://pytorch.org/docs/stable/torch.html#torch.cat\n",
        "        ###     torch.bmm: https://pytorch.org/docs/stable/torch.html#torch.bmm\n",
        "        ###     torch.tanh: https://pytorch.org/docs/stable/torch.html#torch.tanh\n",
        "        ###     torch.squeeze: https://pytorch.org/docs/stable/torch.html#torch.squeeze\n",
        "        ###     torch.unsqueeze: https://pytorch.org/docs/stable/torch.html#torch.unsqueeze\n",
        "\n",
        "        ### YOUR CODE HERE (~6 lines)\n",
        "        \n",
        "        decoder_state = self.rnn(ybar_t, decoder_state)\n",
        "        h, c = decoder_state\n",
        "        attention_w = self.attention(h, encoder_outputs, encoder_output_masks)\n",
        "        attention_w = torch.bmm(attention_w.unsqueeze(dim=1), encoder_outputs).squeeze(1)\n",
        "\n",
        "        u_t = torch.cat((h, attention_w), dim = 1)\n",
        "        o_t = torch.tanh(self.W_u(u_t))\n",
        "\n",
        "        ## END OF YOUR CODE\n",
        "        \n",
        "        return decoder_state, o_t\n",
        "    \n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention,self).__init__()\n",
        "        \n",
        "        ### TODO Initialize variables: \n",
        "        #               self.W_outputs: nn.Linear layer without bias (W_outputs describled in the PDF); \n",
        "        #               self.W_combined: nn.Linear layer without bias (W_combined describled in the PDF)\n",
        "        #               self.W_alignment: nn.Parameter layer (W_alignment describled in the PDF)\n",
        "\n",
        "        # For the use of nn.Linear function, please refer to https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n",
        "        #   In nn.Linear function, the matrix multiplication is a transposed version of the Eq.(1) in description PDF.\n",
        "        # For the use of nn.Parameter function, please refer to https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\n",
        "        #   This is used as \"weights\" matrix for the alignment scores\n",
        "        \n",
        "        ### YOUR CODE HERE (4~6 lines)\n",
        "\n",
        "        self.W_outputs = nn.Linear(2 * hidden_size, hidden_size, bias=False)\n",
        "        self.W_combined = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.W_alignment = nn.Parameter(torch.FloatTensor(hidden_size, 1))\n",
        "        \n",
        "        ### END OF YOUR CODE    \n",
        "        \n",
        "    def forward(self, decoder_hiddens, encoder_outputs, encoder_output_masks):\n",
        "        '''\n",
        "        compute the attention weights \\alpha_t in the PDF\n",
        "        param:\n",
        "            h: torch.FloatTensor of shape (batch_size, hidden_size)\n",
        "            encoder_outputs: torch.FloatTensosr of shape ( batch_size, max_len_in_batch, 2*hidden_size); \"max_len_in_batch\" is the max length in a batch. It is less than \"max_len\".\n",
        "            encoder_output_masks: torch.BoolTensor of shape (batch_size, max_len), specifying which positions are pad tokens. False -- pad token; True -- not pad token\n",
        "        return:\n",
        "            attn_weights: torch.FloatTensor of shape (batch_size, max_len_in_batch)\n",
        "        '''\n",
        "        \n",
        "\n",
        "        decoder_hiddens = decoder_hiddens.unsqueeze(1)                 ### (batch_size, hidden_size, 1)\n",
        "        max_len_in_batch = encoder_outputs.size()[1]\n",
        "\n",
        "        ### TODO:\n",
        "        ###     1. apply linear transformation \"W_outputs\" to \"encoder_outputs\"; the result has  a shape (batch_size, max_len_in_batch, hidden_size)\n",
        "        ###     2. add the \"decoder_hiddens\" and the \"encoder_outputs\" together; the result has  a shape (batch_size, max_len_in_batch + 1, hidden_size)\n",
        "        ###     3. apply linear transformation \"W_combined\" to \"concatenated outputs\"; the result has  a shape (batch_size, max_len_in_batch + 1, hidden_size)\n",
        "        ###     4. apply Tanh function to the linear-transformed \"concatenated outputs\"; the result has  a shape (batch_size, max_len_in_batch + 1, hidden_size)\n",
        "        ###     5. apply torch.matmul to the result of step 4 and \"W_alignment\", yielding score e_t of shape (batch_size, max_len_in_batch, 1);\n",
        "        ###        squeeze e_t in the last dimension\n",
        "        ###     6. apply torch.Tensor.masked_fill_() function to \"e_t\"; the parameters of this function are Bool tensor \"encoder_output_masks\" and a constant \"-float('inf')\";\n",
        "        ###        before \"torch.Tensor.masked_fill_()\" function, this \"encoder_output_masks\" should be sliced to have a shape (batch_size, max_len_in_batch) (Only the first max_len_in_batch columns will be kept)\n",
        "        ###     7. apply \"F.softmax()\" function to \"e_t\", yielding \"alpha_t\" of shape (batch_size, max_len_in_batch)\n",
        "\n",
        "        ### You may use these functions in your implementation:\n",
        "        ###     torch.matmul: https://pytorch.org/docs/stable/generated/torch.matmul.html\n",
        "        ###     torch.squeeze: https://pytorch.org/docs/stable/torch.html#torch.squeeze\n",
        "        ###     torch.Tensor.masked_fill_: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.masked_fill_\n",
        "        ###     F.softmax: https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.softmax\n",
        "        \n",
        "        ### YOUR CODE HERE (4~6 lines)\n",
        "        \n",
        "        encoder_outputs = self.W_outputs(encoder_outputs)\n",
        "\n",
        "        _sum = decoder_hiddens + encoder_outputs\n",
        "        linear_sum = self.W_combined(_sum)\n",
        "        tanh_sum = torch.tanh(linear_sum)\n",
        "        e_t = torch.matmul(tanh_sum, self.W_alignment).squeeze(-1)\n",
        "        \n",
        "        encoder_output_masks = encoder_output_masks[:, 0:max_len_in_batch]\n",
        "        encoder_output_masks = ~encoder_output_masks\n",
        "        e_t.masked_fill_(encoder_output_masks, -float('inf'))\n",
        "        attn_weights = F.softmax(e_t, 1)\n",
        "\n",
        "        ### END OF YOUR CODE\n",
        "        \n",
        "        return attn_weights\n",
        "\n",
        "class NMT(nn.Module):\n",
        "    def __init__(self, encoder_config, decoder_config):\n",
        "        super(NMT, self).__init__()\n",
        "        self.encoder = Encoder(encoder_config)\n",
        "        self.decoder = Decoder(decoder_config)\n",
        "        self.encoder_config = encoder_config\n",
        "        self.decoder_config = decoder_config\n",
        "        self.W_v = nn.Linear(decoder_config['hidden_size'], decoder_config['vocab_size'])\n",
        "        \n",
        "    def forward(self, src_ids, src_lengths, src_masks, tgt_ids, tgt_lengths, tgt_masks):\n",
        "        # src_ids:(batch_size, max_len)\n",
        "        # src_lengths: (batch_size)\n",
        "        # src_mask: (batch_size, max_len)\n",
        "        # tgt_ids: (batch_size, max_len)\n",
        "        # tgt_lengths: (batch_size)\n",
        "        # tgt_masks: (batch_size, max_len)\n",
        "        \n",
        "        encoder_outputs, decoder_init_hidden = self.encoder(src_ids, src_lengths)\n",
        "        outputs = self.decoder(tgt_ids, tgt_lengths, encoder_outputs, src_masks, decoder_init_hidden)\n",
        "        tgt_unnormalized_score = self.W_v(outputs)\n",
        "        tgt_log_prob = F.log_softmax(tgt_unnormalized_score, dim=-1)\n",
        "        \n",
        "        max_len_batch = torch.max(tgt_lengths)\n",
        "        tgt_masks = tgt_masks[:, :max_len_batch].permute(1, 0) # (l,b)\n",
        "        tgt_ids = tgt_ids.permute(1,0)[:max_len_batch, :] #(l,b)\n",
        "        tgt_words_log_prob = torch.gather(tgt_log_prob, -1, tgt_ids[1:].unsqueeze(-1)).squeeze(-1) * tgt_masks[1:].float()\n",
        "        \n",
        "        tgt_sents_log_prob = torch.sum(tgt_words_log_prob, dim=0)\n",
        "        return tgt_sents_log_prob       #(b)\n",
        "\n",
        "    \n",
        "    def beam_search(self, src_ids, src_length, beam_size):\n",
        "        # src_ids: (batch_size, max_len)\n",
        "        # src_lengths: (1, 1)\n",
        "        # beam_size: int\n",
        "        \n",
        "        STOP_ID = self.decoder_config['en_w2i'][STOP_TOKEN]\n",
        "        max_decode_length = 30\n",
        "        encoder_outputs, decoder_init_hidden = self.encoder(src_ids, src_length)\n",
        "        encoder_output_masks = torch.BoolTensor(np.ones((1,src_length.item()))).to(device)\n",
        "        \n",
        "        START_ID = self.decoder_config['en_w2i']['<s>']\n",
        "        prev_o_t = torch.zeros(1, self.decoder_config['hidden_size']).to(device)\n",
        "        input_beam_nodes = [BeamNode(prev_node=None, prev_hidden=decoder_init_hidden, prev_o_t=prev_o_t , wordID=START_ID, \n",
        "                            score=0, length=1)]\n",
        "\n",
        "        finished_beam = 0\n",
        "        end_beam = []\n",
        "        max_finished_beam = beam_size\n",
        "        while finished_beam < max_finished_beam and input_beam_nodes[0].length < max_decode_length:\n",
        "            cur_hidden = []\n",
        "            cur_o_t = []\n",
        "            prev_scores = []\n",
        "            cur_len = input_beam_nodes[0].length\n",
        "\n",
        "            for n in input_beam_nodes:\n",
        "                y_t = self.decoder.tgt_embedding(torch.LongTensor([n.wordID]).to(device))\n",
        "                y_t = torch.cat((y_t, n.prev_o_t), dim=1)\n",
        "\n",
        "                decoder_hidden, o_t = self.decoder.decode_one_step(y_t, n.prev_hidden, encoder_outputs, encoder_output_masks)\n",
        "                cur_hidden.append(decoder_hidden)\n",
        "                cur_o_t.append(o_t)\n",
        "                prev_scores.append(n.score)\n",
        "            \n",
        "            o_t = torch.stack(cur_o_t, dim=0)\n",
        "            scores = self.W_v(o_t).squeeze(1)\n",
        "            prev_scores = torch.Tensor(prev_scores).unsqueeze(-1).expand_as(scores).to(device)\n",
        "            \n",
        "            assert len(scores.size()) == 2\n",
        "            assert scores.size(0) == len(input_beam_nodes)\n",
        "            assert scores.size(1) == self.decoder_config['vocab_size']\n",
        "\n",
        "            log_prob = F.log_softmax(scores, dim=-1)\n",
        "            cur_score = (log_prob + prev_scores).view(-1)\n",
        "            topk_score, topk_pos = torch.topk(cur_score, beam_size)\n",
        "\n",
        "            node_ids = topk_pos // self.decoder_config['vocab_size']\n",
        "            word_ids = topk_pos % self.decoder_config['vocab_size']\n",
        "\n",
        "            next_nodes = []\n",
        "            for score, node_id, word_id in zip(topk_score, node_ids, word_ids):\n",
        "                score = score.item()\n",
        "                node_id = node_id.item()\n",
        "                word_id = word_id.item()\n",
        "\n",
        "                node = BeamNode(prev_node=input_beam_nodes[node_id], prev_hidden=cur_hidden[node_id], \n",
        "                                prev_o_t=cur_o_t[node_id] , score=score,\n",
        "                                wordID=word_id, length=cur_len+1)\n",
        "\n",
        "                if word_id == STOP_ID:\n",
        "                    beam_size -= 1\n",
        "                    end_beam.append(node)\n",
        "                    finished_beam += 1\n",
        "                else:\n",
        "                    next_nodes.append(node)\n",
        "            \n",
        "            input_beam_nodes = next_nodes\n",
        "            \n",
        "            if cur_len + 1 >= max_decode_length:\n",
        "                end_beam.extend(next_nodes)\n",
        "        \n",
        "        seqs = []\n",
        "        for n in end_beam:\n",
        "            seq = []\n",
        "            score = n.score\n",
        "            while True:\n",
        "                prev_node = n.prev_node\n",
        "                wordID = n.wordID\n",
        "                try:\n",
        "                    word = self.decoder_config['en_i2w'][wordID]\n",
        "                except KeyError:\n",
        "                    word = UNK_TOKEN\n",
        "                # print(word)\n",
        "                seq.append(word)\n",
        "                if prev_node.wordID == START_ID:\n",
        "                    break\n",
        "                n = prev_node\n",
        "            seqs.append(Translation(sent=seq[-1:0:-1], score=score))\n",
        "\n",
        "        return seqs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n29iqTqLv3Z3"
      },
      "source": [
        "# metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WYlGzprdv3Z3"
      },
      "outputs": [],
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available else 'cpu'\n",
        "\n",
        "def eval_ppl(model, dev_iter):\n",
        "    model.eval()\n",
        "    \n",
        "    cum_loss = 0.\n",
        "    cum_tgt_words = 0.\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_data in dev_iter:\n",
        "            batch_data = tuple(t.to(device) for t in batch_data[:-1])\n",
        "            b_src_ids, b_tgt_ids, b_src_len, b_tgt_len, b_src_mask, b_tgt_mask = batch_data\n",
        "            batch_loss = -1 * model(b_src_ids, b_src_len, b_src_mask, b_tgt_ids, b_tgt_len, b_tgt_mask).sum()\n",
        "            cum_loss += batch_loss.item()\n",
        "            b_num_words = b_tgt_len.sum() - b_tgt_len.size(0)\n",
        "            cum_tgt_words += b_num_words\n",
        "        \n",
        "        ppl = np.exp(cum_loss/cum_tgt_words.item())\n",
        "        \n",
        "    model.train()\n",
        "    return ppl\n",
        "\n",
        "def compute_corpus_bleu_score(references, predictions):\n",
        "    # references: List[List[str]]\n",
        "    # prediction: Liset[List[str]]\n",
        "    return corpus_bleu([[ref] for ref in references], predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJKc9vj4v3Z4"
      },
      "source": [
        "# main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mFaExOOHv3Z4"
      },
      "outputs": [],
      "source": [
        "def train(train_iter, dev_iter, encoder_config, decoder_config, epoch):\n",
        "\n",
        "    model = NMT(encoder_config, decoder_config)\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    \n",
        "    best_eval_ppl = float('inf')\n",
        "    \n",
        "    for it in range(epoch):\n",
        "        total_train_loss = 0.\n",
        "        total_train_words = 0\n",
        "        \n",
        "        for batch_data in train_iter:\n",
        "            batch_data = tuple(t.to(device) for t in batch_data[:-1])\n",
        "            b_src_ids, b_tgt_ids, b_src_len, b_tgt_len, b_src_mask, b_tgt_mask = batch_data\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            batch_loss = -1 * model(b_src_ids, b_src_len, b_src_mask, b_tgt_ids, b_tgt_len, b_tgt_mask).sum()\n",
        "            loss = batch_loss / batch_size\n",
        "            \n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_train_loss += batch_loss.item()\n",
        "            total_train_words += b_tgt_len.sum() - b_tgt_len.size(0)\n",
        "            \n",
        "        print('train_loss:{}, train_ppl:{} '.format(total_train_loss/batch_size, np.exp(total_train_loss/total_train_words.item())))\n",
        "        \n",
        "        e_ppl = eval_ppl(model, dev_iter)\n",
        "        if e_ppl < best_eval_ppl:\n",
        "            print('better model found!')   \n",
        "            print('eval_ppl:', e_ppl)\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "            best_eval_ppl = e_ppl\n",
        "\n",
        "def test(model, test_iter):\n",
        "    # support only batch_size = 1\n",
        "    model.eval()\n",
        "    corpus_reference = []\n",
        "    corpus_prediction = []\n",
        "    with torch.no_grad():\n",
        "        for batch_data in test_iter:\n",
        "            # Run Inference\n",
        "            raw_sent = batch_data[-1]\n",
        "            batch_data = tuple(t.to(device) for t in batch_data[:-1])\n",
        "            b_src_ids, b_tgt_ids, b_src_len, b_tgt_len, b_src_mask, b_tgt_mask = batch_data\n",
        "            \n",
        "            seqs = model.beam_search(b_src_ids, b_src_len, 2)\n",
        "            sorted_seqs = sorted(seqs, key=lambda x: x.score, reverse=True)\n",
        "            corpus_prediction.append(sorted_seqs[0].sent)\n",
        "            \n",
        "            # Get Raw Sentence\n",
        "            ref = list(tokenizer(x) for x in raw_sent)\n",
        "            corpus_reference += ref\n",
        "            \n",
        "        bleu = compute_corpus_bleu_score(corpus_reference, corpus_prediction)\n",
        "        print('BLEU score on Test set:{}'.format(bleu))\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONvp_aHSv3Z5",
        "outputId": "566099f0-7112-4edd-f732-940b5de3d89c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss:107934.74969863892, train_ppl:31.51580308173513 \n",
            "better model found!\n",
            "eval_ppl: 12.920242120630345\n",
            "train_loss:65049.0252494812, train_ppl:8.000515187891743 \n",
            "better model found!\n",
            "eval_ppl: 8.400872037758555\n",
            "train_loss:47960.85104227066, train_ppl:4.633100381775522 \n",
            "better model found!\n",
            "eval_ppl: 7.226042409882179\n",
            "train_loss:37530.700488090515, train_ppl:3.3194271462652907 \n",
            "better model found!\n",
            "eval_ppl: 6.799640881773428\n",
            "train_loss:30458.435219049454, train_ppl:2.6477335572254534 \n",
            "better model found!\n",
            "eval_ppl: 6.710091781475826\n",
            "train_loss:25579.061647892, train_ppl:2.265326266740426 \n",
            "train_loss:22321.147998571396, train_ppl:2.041263289248497 \n",
            "train_loss:20076.605898857117, train_ppl:1.899925564018366 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:382: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score on Test set:0.334477168766918\n"
          ]
        }
      ],
      "source": [
        "fr_emb = None\n",
        "en_emb = None\n",
        "\n",
        "encoder_config = {'hidden_size': 256, \n",
        "                  'num_layers': 1, \n",
        "                  'bidirectional':True,\n",
        "                  'vocab_size':  len(fr_w2i),\n",
        "                  'emb_size':300, \n",
        "                  'src_embedding': fr_emb}\n",
        "\n",
        "decoder_config = {'hidden_size': 256,\n",
        "                  'vocab_size': len(en_w2i),\n",
        "                  'emb_size': 300, \n",
        "                  'tgt_embedding': en_emb,\n",
        "                  'en_w2i':en_w2i,\n",
        "                  'en_i2w':en_i2w}\n",
        "max_len = 30\n",
        "batch_size = 32\n",
        "tokenizer = lambda x: x.split()\n",
        "\n",
        "train_dataset = TranslationDataset(train_sent_pairs, fr_w2i, en_w2i, tokenizer, max_len)\n",
        "dev_dataset = TranslationDataset(dev_sent_pairs, fr_w2i, en_w2i, tokenizer, max_len)\n",
        "test_dataset = TranslationDataset(test_sent_pairs, fr_w2i, en_w2i, tokenizer, max_len)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available else 'cpu'\n",
        "epoch = 8\n",
        "\n",
        "train(train_loader, dev_loader, encoder_config, decoder_config, epoch)\n",
        "model = NMT(encoder_config, decoder_config)\n",
        "model.load_state_dict(torch.load(r'best_model.pt'))\n",
        "model = model.to(device)\n",
        "test(model, test_loader)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "1004365-homework3_p2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}