{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50.040 Natural Language Processing (Summer 2022) Homework 2\n",
    "\n",
    "**Due 1st July 2022**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### STUDENT ID: -\n",
    "\n",
    "### Name: Hee Ming Shan (TA)\n",
    "\n",
    "### Students with whom you have discussed (if any): -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Constituency parsing aims to extract a constituency-based parse tree from a sentence that represents \n",
    "its syntactic structure according to a phrase structure grammar.\n",
    "A typical constituency parse tree is shown below:\n",
    "\n",
    "![tree](imgs/parse_tree.png)\n",
    "\n",
    "$S$ is a distinguished start symbol, node labels such as $NP$(noun phrase), $VP$(verb phrase) are non-terminal symbols, leaf labels such as \"a\", \"banana\" are terminal symbols.\n",
    "\n",
    "In this homework, we will implement a constituency parser based on probabilistic context-free grammars (PCFGs) and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We will be using a version of the \n",
    "``Penn Treebank''\n",
    "released in NLTK corpora to induce PCFGs and evaluate our algorithm. \n",
    "The preprocessing code has been provided, do not make any changes to the text\n",
    "and code unless you are requested to do so. Run the code we provide to load the training and\n",
    "test sets as Python lists, it will take ~1 minute.\n",
    "Since we will not tune hyper-parameters in this homework, there will be no need for a development set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCFGs\n",
    "A probabilistic context-free grammar consists of:\n",
    "\n",
    "- A context-free grammar $G=(N, \\ \\Sigma, \\ S, \\ R)$ where $N$ is a finite set of non-terminal symbols, $\\Sigma$ is a finite set of terminal symbols, $R$ is a finite set of rules (e.g., $NP \\rightarrow NP \\ PP$), $S \\in N$ is the start symbol.\n",
    "- One parameter $q(A \\rightarrow \\beta)$ for each rule $A \\rightarrow \\beta$ in $R$. Since the grammar is in Chomsky normal form, there are only two types of rules: $A \\rightarrow B \\ C$ and $A \\rightarrow \\alpha$, where $A$, $B$, $C \\in N$, $\\alpha \\in \\Sigma$.\n",
    "\n",
    "We can estimate the parameter $q(A \\rightarrow \\beta)$ using maximum likelihood estimation:\n",
    "\\begin{equation}\n",
    "\tq_{MLE}(A \\rightarrow \\beta) = \\frac {count(A \\rightarrow \\beta)}{count(A)}\n",
    "\\end{equation}\n",
    "where $count(A \\rightarrow \\beta)$ refers to the number of occurences of \n",
    "the rule $A \\rightarrow \\beta$ in all \n",
    "the parse trees in the training set, and  $count(A)$  refers to the number of occurences of\n",
    "the non-terminal symbol $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import Counter\n",
    "from nltk.tree import Tree\n",
    "from nltk import Nonterminal\n",
    "from nltk.corpus import LazyCorpusLoader, BracketParseCorpusReader\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_leave_lower(tree_string):\n",
    "    if isinstance(tree_string, Tree):\n",
    "        tree = tree_string\n",
    "    else:\n",
    "        tree = Tree.fromstring(tree_string)\n",
    "    for idx, _ in enumerate(tree.leaves()):\n",
    "        tree_location = tree.leaf_treeposition(idx)\n",
    "        non_terminal = tree[tree_location[:-1]]\n",
    "        non_terminal[0] = non_terminal[0].lower()\n",
    "    return tree\n",
    "\n",
    "def get_train_test_data():\n",
    "    '''\n",
    "    Load training and test set from nltk corpora\n",
    "    '''\n",
    "    train_num = 3900\n",
    "    test_index = range(10)\n",
    "    treebank = LazyCorpusLoader('treebank/combined', BracketParseCorpusReader, r'wsj_.*\\.mrg')\n",
    "    cnf_train = treebank.parsed_sents()[:train_num]\n",
    "    cnf_test = [treebank.parsed_sents()[i+train_num] for i in test_index]\n",
    "    #Convert to Chomsky norm form, remove auxiliary labels\n",
    "    cnf_train = [convert2cnf(t) for t in cnf_train]\n",
    "    cnf_test = [convert2cnf(t) for t in cnf_test]\n",
    "    return cnf_train, cnf_test\n",
    "def convert2cnf(original_tree):\n",
    "    '''\n",
    "    Chomsky norm form\n",
    "    '''\n",
    "    tree = copy.deepcopy(original_tree)\n",
    "    \n",
    "    #Remove cases like NP->DT, VP->NP\n",
    "    tree.collapse_unary(collapsePOS=True, collapseRoot=True)\n",
    "    #Convert to Chomsky\n",
    "    tree.chomsky_normal_form()\n",
    "    \n",
    "    tree = set_leave_lower(tree)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GET TRAIN/TEST DATA\n",
    "cnf_train, cnf_test = get_train_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ\n",
      "    (NP (NNP pierre) (NNP vinken))\n",
      "    (NP-SBJ|<,-ADJP-,>\n",
      "      (, ,)\n",
      "      (NP-SBJ|<ADJP-,>\n",
      "        (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
      "        (, ,))))\n",
      "  (S|<VP-.>\n",
      "    (VP\n",
      "      (MD will)\n",
      "      (VP\n",
      "        (VB join)\n",
      "        (VP|<NP-PP-CLR-NP-TMP>\n",
      "          (NP (DT the) (NN board))\n",
      "          (VP|<PP-CLR-NP-TMP>\n",
      "            (PP-CLR\n",
      "              (IN as)\n",
      "              (NP\n",
      "                (DT a)\n",
      "                (NP|<JJ-NN> (JJ nonexecutive) (NN director))))\n",
      "            (NP-TMP (NNP nov.) (CD 29))))))\n",
      "    (. .)))\n"
     ]
    }
   ],
   "source": [
    "cnf_train[0].pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "To  better  understand  PCFG,  let's  consider  the  first  parse  tree  in  the training data \"cnf_train\" as an example.  Run the code we have provided for you and then writedown the roles of.productions(), .rhs(), .lhs(), .leaves()in the ipynb notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S -> NP-SBJ S|<VP-.>, NP-SBJ -> NP NP-SBJ|<,-ADJP-,>, NP -> NNP NNP, NNP -> 'pierre', NNP -> 'vinken', NP-SBJ|<,-ADJP-,> -> , NP-SBJ|<ADJP-,>, , -> ',', NP-SBJ|<ADJP-,> -> ADJP ,, ADJP -> NP JJ, NP -> CD NNS, CD -> '61', NNS -> 'years', JJ -> 'old', , -> ',', S|<VP-.> -> VP ., VP -> MD VP, MD -> 'will', VP -> VB VP|<NP-PP-CLR-NP-TMP>, VB -> 'join', VP|<NP-PP-CLR-NP-TMP> -> NP VP|<PP-CLR-NP-TMP>, NP -> DT NN, DT -> 'the', NN -> 'board', VP|<PP-CLR-NP-TMP> -> PP-CLR NP-TMP, PP-CLR -> IN NP, IN -> 'as', NP -> DT NP|<JJ-NN>, DT -> 'a', NP|<JJ-NN> -> JJ NN, JJ -> 'nonexecutive', NN -> 'director', NP-TMP -> NNP CD, NNP -> 'nov.', CD -> '29', . -> '.'] <class 'nltk.grammar.Production'>\n"
     ]
    }
   ],
   "source": [
    "rules = cnf_train[0].productions()\n",
    "print(rules, type(rules[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((NP-SBJ, S|<VP-.>), nltk.grammar.Nonterminal)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules[0].rhs(), type(rules[0].rhs()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('61',), str)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules[10].rhs(), type(rules[10].rhs()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(S, nltk.grammar.Nonterminal)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules[0].lhs(), type(rules[0].lhs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pierre', 'vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', '29', '.']\n"
     ]
    }
   ],
   "source": [
    "print(cnf_train[0].leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER HERE**\n",
    "- .productions(): \n",
    "Returns the unique grammar rules of this sentence. Each grammar rule maps a single symbol on the “left-hand side” to a sequence of symbols on the “right-hand side”. Since the grammer is in Chomsky normal form (CNF), the left-hand side must be a non-terminal symbol and the right-hand side is either a terminal symbol or two non-terminal symbols.\n",
    "- .rhs():\n",
    "Returns either a terminal symbol or two non-terminal symbols on the right-hand side of the grammar rule.\n",
    "- .lhs():\n",
    "Returns the non-terminal symbol on the left-hand side of the grammar rule.\n",
    "- .leaves():\n",
    "Returns the leaves of the parse tree which are all terminal symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "To count the number of unique rules, nonterminals and terminals, please implement functions **collect_rules, collect_nonterminals, collect_terminals**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_rules(train_data):\n",
    "    '''\n",
    "    Collect the rules that appear in data.\n",
    "    params:\n",
    "        train_data: list[Tree] --- list of Tree objects\n",
    "    return:\n",
    "        rules: list[nltk.grammar.Production] --- list of rules (Production objects)\n",
    "        rules_counts: Counter object --- a dictionary that maps one rule (nltk.Nonterminal) to its number of \n",
    "                                         occurences (int) in train data.\n",
    "    '''\n",
    "    rules = list()\n",
    "    rules_counts = Counter()\n",
    "    ### YOUR CODE HERE\n",
    "    rules = [rule for tree in train_data for rule in tree.productions()]\n",
    "    rule_counts = Counter(rules)\n",
    "    ### YOUR CODE HERE\n",
    "    return rules, rule_counts\n",
    "\n",
    "def collect_nonterminals(rules):\n",
    "    '''\n",
    "    collect nonterminals that appear in the rules\n",
    "    params:\n",
    "        rules: list[nltk.grammar.Production] --- list of rules (Production objects)\n",
    "    return:\n",
    "        nonterminals: set(nltk.Nonterminal) --- set of nonterminals \n",
    "    '''\n",
    "    ### YOUR CODE HERE \n",
    "    nonterminals_list = [rule.lhs() for rule in rules if type(rule.lhs()) is Nonterminal]\n",
    "    nonterminals = set(nonterminals_list)\n",
    "    ### END OF YOUR CODE\n",
    "    return nonterminals\n",
    "\n",
    "def collect_terminals(rules):\n",
    "    '''\n",
    "    collect terminals that appear in the rules\n",
    "    params:\n",
    "        rules: list[nltk.grammar.Production] --- list of rules (Production objects)\n",
    "    return:\n",
    "        terminals: set of strings --- set of terminals    \n",
    "    '''\n",
    "    terminals = list()\n",
    "    ### YOUR CODE HERE\n",
    "    terminals_list = [r for rule in rules for r in rule.rhs() if type(r) is str]\n",
    "    terminals = set(terminals_list)\n",
    "    ### END OF YOUR CODE\n",
    "    return terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rules, train_rules_counts = collect_rules(cnf_train)\n",
    "nonterminals = collect_nonterminals(train_rules)\n",
    "terminals = collect_terminals(train_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196646, 31656, 11367, 7869)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_rules), len(set(train_rules)), len(terminals), len(nonterminals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(, -> ',', 4876), (DT -> 'the', 4726), (. -> '.', 3814), (PP -> IN NP, 3273), (S|<VP-.> -> VP ., 3003)]\n"
     ]
    }
   ],
   "source": [
    "print(train_rules_counts.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Implement the function **build_pcfg** which builds a dictionary that stores the terminal rules and nonterminal rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pcfg(rules_counts):\n",
    "    '''\n",
    "    Build a dictionary that stores the terminal rules and nonterminal rules.\n",
    "    param:\n",
    "        rules_counts: Counter object --- a dictionary that maps one rule to its number of occurences in train data.\n",
    "    return:\n",
    "        rules_dict: dict(dict(dict)) --- a dictionary has a form like:\n",
    "                    rules_dict = {'terminals':{'NP':{'the':1000,'an':500}, 'ADJ':{'nice':500,'good':100}},\n",
    "                                  'nonterminals':{'S':{'NP@VP':1000},'NP':{'NP@NP':540}}}\n",
    "    When building \"rules_dict\", you need to use \"lhs()\", \"rhs()\" funtion and convert Nonterminal to str.\n",
    "    All the keys in the dictionary are of type str.\n",
    "    '@' is used as a special symbol to split left and right nonterminal strings.\n",
    "    '''\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    rules_dict = dict()\n",
    "    ### rules_dict['terminals'] contains rules like \"NP->'the'\"\n",
    "    ### rules_dict['nonterminals'] contains rules like \"S->NP@VP\"\n",
    "    rules_dict['terminals'] = defaultdict(dict)\n",
    "    rules_dict['nonterminals'] = defaultdict(dict)\n",
    "    for rule, counts in rules_counts.items():\n",
    "        lhs = rule.lhs().symbol()\n",
    "        rhs = rule.rhs()\n",
    "        \n",
    "        length = len(rhs)\n",
    "        \n",
    "        if length > 1:\n",
    "            head, tail = rhs[0].symbol(), rhs[1].symbol()\n",
    "            new_rhs = head + '@' + tail\n",
    "            rules_dict['nonterminals'][lhs][new_rhs] = counts\n",
    "        \n",
    "        elif length == 1:\n",
    "            if isinstance(rhs[0], str):\n",
    "                new_rhs = rhs[0]\n",
    "                rules_dict['terminals'][lhs][new_rhs] = counts\n",
    "            else:\n",
    "                raise ValueError\n",
    "        else:\n",
    "            raise ValueError\n",
    "    ### END OF YOUR CODE\n",
    "    return rules_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rules_dict = build_pcfg(train_rules_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Count and find all the terminal symbols in ''cnf_test'' that never appeared in the PCFG we built. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 {'implicitly', 'authorizes', 'kennedy', 'kurland', 'shared', 'partial', 'constitutional-law', 'laurence', 'procedure', 'scuttle', 'professors', 'spectrum', 'lawmaking', 'tribe'}\n"
     ]
    }
   ],
   "source": [
    "test_leaves = set()\n",
    "for cnf in cnf_test:\n",
    "    test_leaves.update(set(cnf.leaves()))\n",
    "    \n",
    "new_terminals = test_leaves.difference(terminals)\n",
    "print(len(new_terminals), new_terminals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "We can use smoothing techniques to handle these cases. A simple smoothing method is as follows. We first create a new \"unknown\" terminal symbol $unk$.\n",
    "\n",
    "Next, for each original non-terminal symbol $A\\in N$, we add one new rule $A \\rightarrow unk$ to the original PCFG.\n",
    "\n",
    "The smoothed probabilities for all rules can then be estimated as:\n",
    "$$q_{smooth}(A \\rightarrow \\beta) = \\frac {count(A \\rightarrow \\beta)}{count(A)+1}$$\n",
    "$$q_{smooth}(A \\rightarrow unk) = \\frac {1}{count(A)+1}$$\n",
    "where $|V|$ is the count of unique terminal symbols.\n",
    "\n",
    "\n",
    "Implement the function **smooth_rules_prob** which returns the smoothed rule probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_rules_prob(rules_counts):\n",
    "    '''\n",
    "    params:\n",
    "        rules_counts: dict(dict(dict)) --- a dictionary has a form like:\n",
    "                      rules_counts = {'terminals':{'NP':{'the':1000,'an':500}, 'ADJ':{'nice':500,'good':100}},\n",
    "                                      'nonterminals':{'S':{'NP@VP':1000},'NP':{'NP@NP':540}}}\n",
    "    \n",
    "    return:\n",
    "        rules_prob: dict(dict(dict)) --- a dictionary that has a form like:\n",
    "                               rules_prob = {'terminals':{'NP':{'the':0.6,'an':0.3, '<unk>':0.1},\n",
    "                                                          'ADJ':{'nice':0.6,'good':0.3,'<unk>':0.1},\n",
    "                                                          'S':{'<unk>':0.01}}}\n",
    "                                             'nonterminals':{'S':{'NP@VP':0.99}}\n",
    "    '''\n",
    "    rules_prob = copy.deepcopy(rules_counts)\n",
    "    unk = '<unk>'\n",
    "    ### YOUR CODE HERE\n",
    "    for n in rules_counts['nonterminals'].keys():\n",
    "        den = sum(rules_counts['nonterminals'][n].values())\n",
    "        \n",
    "        if n in rules_counts['terminals'].keys():\n",
    "            den += sum(rules_counts['terminals'][n].values())\n",
    "        \n",
    "        for k,num in rules_counts['nonterminals'][n].items():\n",
    "            rules_prob['nonterminals'][n][k] = float(num / (den+1))\n",
    "        rules_prob['terminals'][n]['<unk>'] = 1.0 / (den+1)\n",
    "    \n",
    "    for t in rules_counts['terminals'].keys():\n",
    "        den = sum(rules_counts['terminals'][t].values())\n",
    "        \n",
    "        if t in rules_counts['nonterminals'].keys():\n",
    "            den += sum(rules_counts['nonterminals'][t].values())\n",
    "        \n",
    "        for k, num in rules_counts['terminals'][t].items():\n",
    "            rules_prob['terminals'][t][k] = float(num / (den+1))\n",
    "        rules_prob['terminals'][t]['<unk>'] = 1.0/(den+1)\n",
    "    \n",
    "    ### END OF YOUR CODE\n",
    "    return rules_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_rules_prob = smooth_rules_prob(train_rules_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1300172371337109\n",
      "0.025240088648116228\n",
      "0.039506305917861376\n",
      "{'<unk>': 5.389673385792821e-05}\n",
      "0.1300172371337109\n"
     ]
    }
   ],
   "source": [
    "print(s_rules_prob['nonterminals']['S']['NP-SBJ@S|<VP-.>'])\n",
    "print(s_rules_prob['nonterminals']['S']['NP-SBJ-1@S|<VP-.>'])\n",
    "print(s_rules_prob['nonterminals']['NP']['NNP@NNP'])\n",
    "print(s_rules_prob['terminals']['NP'])\n",
    "print(s_rules_prob['nonterminals']['S']['NP-SBJ@S|<VP-.>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11367"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(terminals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "Estimate the probability of \"cnf_test[7]\", which is  the 7th  parse  tree  in  the testing data \"cnf_test\" by using \"s_rules_prob\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.492135946536645e-54\n"
     ]
    }
   ],
   "source": [
    "q6_test_rules, _ = collect_rules([cnf_test[7]])\n",
    "prob = 1\n",
    "for rule in q6_test_rules:\n",
    "    if len(rule.rhs()) == 1:\n",
    "        if rule.rhs()[0] not in s_rules_prob['terminals'][rule.lhs().symbol()]:\n",
    "            prob *= s_rules_prob['terminals'][rule.lhs().symbol()]['<unk>'] \n",
    "        else:\n",
    "            prob *= s_rules_prob['terminals'][rule.lhs().symbol()][rule.rhs()[0]]\n",
    "    else:\n",
    "        prob *= s_rules_prob['nonterminals'][rule.lhs().symbol()][rule.rhs()[0].symbol()+'@'+rule.rhs()[1].symbol()]\n",
    "\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CKY Algorithm\n",
    "\n",
    "Similar to the Viterbi algorithm, the CKY algorithm is a dynamic-programming algorithm. Given a PCFG $G=(N, \\ \\Sigma, \\ S, \\ R)$, we can use the CKY algorithm described in class to find the highest scoring parse tree for a sentence. \n",
    "\n",
    "First, let us complete the *CKY* function from scratch using only Python built-in functions and the Numpy package. \n",
    "\n",
    "The output should be two dictionaries $\\pi$ and $bp$, which store the optimal probability and backpointer information respectively.\n",
    "\n",
    "Given a sentence $w_0, w_1, ...,w_{n-1}$,  $\\pi(i, k, X)$, $bp(i, k, X)$ refer to the highest score and backpointer for the (partial) parse tree that has the root X (a non-terminal symbol) and covers the word span $w_i, ..., w_{k-1}$, where $0 \\le i < k \\le n$. Note that a backpointer includes both the best grammar rule chosen and the best split point.\n",
    "![tree](imgs/parse_tree.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "Implement **CKY** function and run the test code to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CKY(sent, rules_prob):\n",
    "    '''\n",
    "    params:\n",
    "        sent: list[str] --- a list of strings\n",
    "        rules_prob: dict(dict(dict)) --- a dictionary that has a form like:\n",
    "                                           rules_prob = {'terminals':{'NP':{'the':0.6,'an':0.3, '<unk>':0.1},\n",
    "                                                                      'ADJ':{'nice':0.6,'good':0.3,'<unk>':0.1},\n",
    "                                                                      'S':{'<unk>':0.01}}}\n",
    "                                                         'nonterminals':{'S':{'NP@VP':0.99}}\n",
    "    return:\n",
    "        score: dict(dict) --- score[(i,i+span)][X] represents the highest score for the (partial) parse tree that has the root X\n",
    "                          across words w_i, w_{i+1},..., w_{i+span-1}.\n",
    "        back: dict(dict) --- back[(i,i+span)][root] = (split , left_child, right_child); split: int; \n",
    "                         left_child: str; right_child: str. \n",
    "    '''\n",
    "    \n",
    "    score = defaultdict(dict)\n",
    "    back = defaultdict(dict)\n",
    "    sent_len = len(sent)\n",
    "    '''\n",
    "    for i in range(sent_len):\n",
    "        for x in rules_prob['terminals'].keys():\n",
    "            score[(i, i+1)][x] = rules_prob['terminals'][x].get(sent[i], 0)\n",
    "            if score[(i, i+1)][x] > 0:\n",
    "                back[(i, i+1)][x] = (None, sent[i],  None)\n",
    "                            \n",
    "    for span in range(2, sent_len+1):\n",
    "        for begin in range(0, sent_len - span + 1):\n",
    "            end = begin + span \n",
    "            for split in range(begin+1, end):\n",
    "                for x in rules_prob['nonterminals'].keys():\n",
    "                    for y, p in rules_prob['nonterminals'][x].items():\n",
    "                        [lhs, rhs] = y.split('@')\n",
    "                        prob = p * score[(begin, split)].get(lhs, 0) * score[(split, end)].get(rhs, 0)\n",
    "                        if prob > score[(begin, end)].get(x, 0):\n",
    "                            score[(begin, end)][x] = prob\n",
    "                            back[(begin, end)][x] = (split, lhs, rhs)\n",
    "'''\n",
    "    terminals = rules_prob['terminals']\n",
    "    nonterminals = rules_prob['nonterminals']\n",
    "\n",
    "    for i in range(0, sent_len):\n",
    "        for A in terminals.keys():\n",
    "            if sent[i] in terminals[A].keys():\n",
    "                score[(i,i+1)][A] = terminals[A][sent[i]]\n",
    "                back[(i,i+1)][A] = ('terminal', sent[i])\n",
    "\n",
    "    for span in range(2, sent_len+1):\n",
    "        for begin in range(0, sent_len - span + 1):\n",
    "            end = begin + span\n",
    "            for split in range(begin+1, end):\n",
    "                for A in nonterminals.keys():                    \n",
    "                    for rhs in nonterminals[A].keys():         \n",
    "                        B,C = rhs.split('@')\n",
    "                        if B in score[(begin, split)].keys() and C in score[(split, end)].keys():\n",
    "                            new_score = nonterminals[A][rhs] * score[(begin, split)][B] * score[(split, end)][C]\n",
    "                            if A not in score[(begin, end)] or new_score > score[(begin, end)][A]:\n",
    "                                score[(begin, end)][A] = new_score\n",
    "                                back[(begin, end)][A] = (split, B, C)\n",
    "    return score, back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "Implement **build_tree** function to reconstruct the parse tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(back, root):\n",
    "    '''\n",
    "    Build the tree recursively.\n",
    "    params:\n",
    "        back: dict() --- back[(i,i+span)][X] = (split , left_child, right_child); split:int; left_child: str; right_child: str.\n",
    "        root: tuple() --- (begin, end, nonterminal_symbol), e.g., (0, 10, 'S')\n",
    "    return:\n",
    "        tree: nltk.tree.Tree\n",
    "    '''\n",
    "    \n",
    "    begin = root[0]\n",
    "    end = root[1]\n",
    "    root_label = root[2]\n",
    "    \n",
    "    split_info = back[(begin,end)][root_label]\n",
    "\n",
    "    if len(split_info) > 2:\n",
    "        split, left_child, right_child = split_info\n",
    "        \n",
    "        left_root = (begin, split, left_child)\n",
    "        right_root = (split, end, right_child)\n",
    "        \n",
    "        left_tree = build_tree(back, left_root)\n",
    "        right_tree = build_tree(back, right_root)\n",
    "        \n",
    "        tree = nltk.tree.Tree(root_label, [left_tree, right_tree])\n",
    "    else:\n",
    "        _, left_child = split_info\n",
    "        tree = nltk.tree.Tree(root_label, [left_child])\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9 & 10\n",
    "- Use **CKY** function to compute the max probability for the sentence of \"cnf_test[7]\", which is the 7th parse tree in the testing data \"cnf_test\".\n",
    "- Generate and display the parse tree of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = cnf_test[7].leaves()\n",
    "sent = [w.lower() if w in terminals else '<unk>' for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.690181277855939e-49\n"
     ]
    }
   ],
   "source": [
    "score, back = CKY(sent, s_rules_prob)\n",
    "max_prob = score[(0, len(sent))]['S']\n",
    "print(max_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ\n",
      "    (DT the)\n",
      "    (NP-SBJ|<CD-NNS-RB> (CD two) (NP-SBJ|<NNS-RB> <unk>)))\n",
      "  (VP\n",
      "    (VBD said)\n",
      "    (SBAR\n",
      "      (-NONE- 0)\n",
      "      (S\n",
      "        (NP-SBJ-1\n",
      "          (DT the)\n",
      "          (NP-SBJ-1|<NNP-NNP-NNP-NNP-NN>\n",
      "            (NNP constitution)\n",
      "            (NP-SBJ-1|<NNP-NNP-NNP-NN> <unk>)))\n",
      "        (S|<NP-TMP-VP-.>\n",
      "          (NP-TMP (DT the) (NN president))\n",
      "          (S|<VP-.>\n",
      "            (VP\n",
      "              (TO to)\n",
      "              (VP\n",
      "                (VB veto)\n",
      "                (NP\n",
      "                  (NP (JJ entire) (NNS bills))\n",
      "                  (NP|<,-NP>\n",
      "                    (, ,)\n",
      "                    (NP\n",
      "                      (QP (RB not) (QP|<JJ-IN-CD> <unk>))\n",
      "                      (NNS measures))))))\n",
      "            (. .)))))))\n"
     ]
    }
   ],
   "source": [
    "optimal_tree = build_tree(back, (0, len(sent), 'S'))\n",
    "optimal_tree.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11\n",
    "Run the remaining code to test your model on test data \"cnf_test\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_leave_index(tree):\n",
    "    '''\n",
    "    Label the leaves of the tree with indexes\n",
    "    Arg:\n",
    "        tree: original tree, nltk.tree.Tree\n",
    "    Return:\n",
    "        tree: preprocessed tree, nltk.tree.Tree\n",
    "    '''\n",
    "    for idx, _ in enumerate(tree.leaves()):\n",
    "        tree_location = tree.leaf_treeposition(idx)\n",
    "        non_terminal = tree[tree_location[:-1]]\n",
    "        non_terminal[0] = non_terminal[0] + \"_\" + str(idx)\n",
    "    return tree\n",
    "\n",
    "def get_nonterminal_bracket(tree):\n",
    "    '''\n",
    "    Obtain the constituent brackets of a tree\n",
    "    Arg:\n",
    "        tree: tree, nltk.tree.Tree\n",
    "    Return:\n",
    "        nonterminal_brackets: constituent brackets, set\n",
    "    '''\n",
    "    nonterminal_brackets = set()\n",
    "    for tr in tree.subtrees():\n",
    "        label = tr.label()\n",
    "        #print(tr.leaves())\n",
    "        if len(tr.leaves()) == 0:\n",
    "            continue\n",
    "        start = tr.leaves()[0].split('_')[-1]\n",
    "        end = tr.leaves()[-1].split('_')[-1]\n",
    "        if start != end:\n",
    "            nonterminal_brackets.add(label+'-('+start+':'+end+')')\n",
    "    return nonterminal_brackets\n",
    "\n",
    "def word2lower(w, terminals):\n",
    "    '''\n",
    "    Map an unknow word to \"unk\"\n",
    "    '''\n",
    "    return w.lower() if w in terminals else '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Test Tree: 1\n",
      "Constituent number in the predicted tree: 20\n",
      "Constituent number in the gold tree: 20\n",
      "Correct constituent number: 14\n",
      "####################\n",
      "Test Tree: 2\n",
      "Constituent number in the predicted tree: 54\n",
      "Constituent number in the gold tree: 54\n",
      "Correct constituent number: 30\n",
      "####################\n",
      "Test Tree: 3\n",
      "Constituent number in the predicted tree: 30\n",
      "Constituent number in the gold tree: 30\n",
      "Correct constituent number: 23\n",
      "####################\n",
      "Test Tree: 4\n",
      "Constituent number in the predicted tree: 17\n",
      "Constituent number in the gold tree: 17\n",
      "Correct constituent number: 16\n",
      "####################\n",
      "Test Tree: 5\n",
      "Constituent number in the predicted tree: 32\n",
      "Constituent number in the gold tree: 32\n",
      "Correct constituent number: 26\n",
      "####################\n",
      "Test Tree: 6\n",
      "Constituent number in the predicted tree: 40\n",
      "Constituent number in the gold tree: 40\n",
      "Correct constituent number: 18\n",
      "####################\n",
      "Test Tree: 7\n",
      "Constituent number in the predicted tree: 22\n",
      "Constituent number in the gold tree: 22\n",
      "Correct constituent number: 7\n",
      "####################\n",
      "Test Tree: 8\n",
      "Constituent number in the predicted tree: 18\n",
      "Constituent number in the gold tree: 18\n",
      "Correct constituent number: 6\n",
      "####################\n",
      "Test Tree: 9\n",
      "Constituent number in the predicted tree: 28\n",
      "Constituent number in the gold tree: 28\n",
      "Correct constituent number: 16\n",
      "####################\n",
      "Test Tree: 10\n",
      "Constituent number in the predicted tree: 40\n",
      "Constituent number in the gold tree: 40\n",
      "Correct constituent number: 8\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "pred_count = 0\n",
    "gold_count = 0\n",
    "for i, t in enumerate(cnf_test):\n",
    "    #Protect the original tree \n",
    "    t = copy.deepcopy(t)\n",
    "    sent = t.leaves()  \n",
    "    #Map the unknow words to \"unk\"\n",
    "    sent = [word2lower(w.lower(), terminals) for w in sent]\n",
    "    \n",
    "    #CKY algorithm\n",
    "    score, back = CKY(sent, s_rules_prob)\n",
    "    candidate_tree = build_tree(back, (0, len(sent), 'S'))\n",
    "    \n",
    "    #Extract constituents from the gold tree and predicted tree\n",
    "    pred_tree = set_leave_index(candidate_tree)\n",
    "    pred_brackets = get_nonterminal_bracket(pred_tree)\n",
    "    \n",
    "    #Count correct constituents\n",
    "    pred_count += len(pred_brackets)\n",
    "    gold_tree = set_leave_index(t)\n",
    "    gold_brackets = get_nonterminal_bracket(gold_tree)\n",
    "    gold_count += len(gold_brackets)\n",
    "    current_correct_num = len(pred_brackets.intersection(gold_brackets))\n",
    "    correct_count += current_correct_num\n",
    "    \n",
    "    print('#'*20)\n",
    "    print('Test Tree:', i+1)\n",
    "    print('Constituent number in the predicted tree:', len(pred_brackets))\n",
    "    print('Constituent number in the gold tree:', len(gold_brackets))\n",
    "    print('Correct constituent number:', current_correct_num)\n",
    "\n",
    "recall = correct_count/gold_count\n",
    "precision = correct_count/pred_count\n",
    "f1 = 2*recall*precision/(recall+precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall precision: 0.545, recall: 0.545, f1: 0.545\n"
     ]
    }
   ],
   "source": [
    "print('Overall precision: {:.3f}, recall: {:.3f}, f1: {:.3f}'.format(precision, recall, f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
